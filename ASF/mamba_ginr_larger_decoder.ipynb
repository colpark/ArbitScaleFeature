{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA-GINR with LARGER DECODER\n",
    "\n",
    "## Changes from previous version:\n",
    "- âœ… Gaussian Fourier features (same as before)\n",
    "- âœ… Correct jittering implementation (same as before)\n",
    "- âœ… Separate modulation/decoding coordinates (same as before)\n",
    "- âœ… No jittering at test time (same as before)\n",
    "- ðŸ†• **LARGER DECODER**: hidden_dim=512, num_layers=3 (was 256, 1)\n",
    "- ðŸ†• **Expected improvement**: +3-5 dB PSNR at 128Ã—128 super-resolution\n",
    "\n",
    "## Decoder capacity comparison:\n",
    "- **Small decoder** (previous): 256 dims, 1 layer/scale, ~450K params\n",
    "- **Large decoder** (this notebook): 512 dims, 3 layers/scale, ~4M params\n",
    "- **Increase**: 9x parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Gaussian Fourier Feature Encoding\n",
    "\n",
    "def gaussian_fourier_encode(coords, B_matrix):\n",
    "    \"\"\"\n",
    "    Gaussian Fourier Feature encoding (Rahimi & Recht, 2007)\n",
    "    \n",
    "    Args:\n",
    "        coords: (H, W, 2) or (HW, 2) - normalized coordinates in [0,1]\n",
    "        B_matrix: (n_features, 2) - random frequency matrix\n",
    "    \n",
    "    Returns:\n",
    "        features: (HW, 2*n_features) - Fourier features\n",
    "    \"\"\"\n",
    "    if coords.dim() == 3:\n",
    "        coords = coords.view(-1, coords.shape[-1])\n",
    "    \n",
    "    proj = 2 * np.pi * coords @ B_matrix.T\n",
    "    features = torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "    return features\n",
    "\n",
    "\n",
    "def create_coordinate_grid(H, W, device):\n",
    "    \"\"\"Create normalized coordinate grid in [0,1]\"\"\"\n",
    "    y = torch.linspace(0, 1, H, device=device)\n",
    "    x = torch.linspace(0, 1, W, device=device)\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    coords = torch.stack([yy, xx], dim=-1)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Core Components (BiMamba, Encoder, LP Tokens)\n",
    "\n",
    "class BiMamba(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.forward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.backward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.proj = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_forward = self.forward_mamba(x)\n",
    "        x_backward = self.backward_mamba(torch.flip(x, dims=[1]))\n",
    "        x_backward = torch.flip(x_backward, dims=[1])\n",
    "        x = torch.cat([x_forward, x_backward], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, patch_size=2, in_channels=3, dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_channels, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        p = self.patch_size\n",
    "        x = einops.rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LearnablePositionTokens(nn.Module):\n",
    "    def __init__(self, num_tokens=256, dim=256):\n",
    "        super().__init__()\n",
    "        self.tokens = nn.Parameter(torch.randn(num_tokens, dim) * 0.02)\n",
    "\n",
    "    def forward(self, B):\n",
    "        return einops.repeat(self.tokens, 'n d -> b n d', b=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Cross-Attention and Residual Block\n",
    "\n",
    "class SharedTokenCrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention with spatial bias\"\"\"\n",
    "    def __init__(self, query_dim, context_dim=None, heads=2, dim_head=64):\n",
    "        super().__init__()\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context, bias=None):\n",
    "        B, HW, D = x.shape\n",
    "        H = self.heads\n",
    "        Dh = self.dim_head\n",
    "        D_inner = H * Dh\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(context)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "\n",
    "        q = q.view(B, HW, H, Dh).transpose(1, 2)\n",
    "        k = k.view(B, -1, H, Dh).transpose(1, 2)\n",
    "        v = v.view(B, -1, H, Dh).transpose(1, 2)\n",
    "\n",
    "        sim = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        if bias is not None:\n",
    "            bias = einops.repeat(bias, 'b l n -> b h l n', h=H)\n",
    "            bias = bias.transpose(-2, -1)\n",
    "            sim = sim + bias\n",
    "\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, HW, D_inner)\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for deeper decoder\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim)\n",
    "        self.linear2 = nn.Linear(dim, dim)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return self.act(x + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LARGER LAINR Decoder\n",
    "\n",
    "class LAINRDecoderLarge(nn.Module):\n",
    "    \"\"\"\n",
    "    LARGER LAINR Decoder with increased capacity\n",
    "    \n",
    "    Changes from original:\n",
    "    - hidden_dim: 256 â†’ 512 (2x width)\n",
    "    - num_layers: 1 â†’ 3 (3x depth per scale)\n",
    "    - Residual connections between layers\n",
    "    - Total parameters: ~450K â†’ ~4M (9x increase)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=64, input_dim=2, output_dim=3,\n",
    "                 sigma_q=16, sigma_ls=[128, 32], n_patches=256,\n",
    "                 hidden_dim=512,\n",
    "                 context_dim=256,\n",
    "                 learnable_frequencies=True,\n",
    "                 num_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = len(sigma_ls)\n",
    "        self.n_features = feature_dim // 2\n",
    "        self.patch_num = int(math.sqrt(n_patches))\n",
    "        self.alpha = 10.0\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize Gaussian Fourier frequency matrices\n",
    "        B_q_init = torch.randn(self.n_features, input_dim) / sigma_q\n",
    "        B_ls_init = [torch.randn(self.n_features, input_dim) / sigma_ls[i]\n",
    "                     for i in range(self.layer_num)]\n",
    "\n",
    "        if learnable_frequencies:\n",
    "            self.B_q = nn.Parameter(B_q_init)\n",
    "            self.B_ls = nn.ParameterList([\n",
    "                nn.Parameter(B_ls_init[i]) for i in range(self.layer_num)\n",
    "            ])\n",
    "        else:\n",
    "            self.register_buffer('B_q', B_q_init)\n",
    "            for i in range(self.layer_num):\n",
    "                self.register_buffer(f'B_l_{i}', B_ls_init[i])\n",
    "            self.B_ls = [getattr(self, f'B_l_{i}') for i in range(self.layer_num)]\n",
    "\n",
    "        # Query encoding - LARGER\n",
    "        self.query_lin = nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "        self.modulation_ca = SharedTokenCrossAttention(\n",
    "            query_dim=hidden_dim, context_dim=context_dim, heads=2\n",
    "        )\n",
    "\n",
    "        # Bandwidth encoders - LARGER + DEEPER\n",
    "        self.bandwidth_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[ResidualBlock(hidden_dim) for _ in range(num_layers - 1)]\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        # Modulation projections - LARGER + DEEPER\n",
    "        self.modulation_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[ResidualBlock(hidden_dim) for _ in range(num_layers - 1)]\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        # Hidden value layers - LARGER\n",
    "        self.hv_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[ResidualBlock(hidden_dim) for _ in range(num_layers - 1)]\n",
    "            )\n",
    "            for _ in range(self.layer_num - 1)\n",
    "        ])\n",
    "\n",
    "        # Output layers - LARGER + DEEPER\n",
    "        self.out_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                *[ResidualBlock(hidden_dim) for _ in range(num_layers - 1)],\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def get_patch_index(self, grid, H, W):\n",
    "        \"\"\"Convert coordinates to patch indices\"\"\"\n",
    "        y = grid[:, 0]\n",
    "        x = grid[:, 1]\n",
    "        row = (y * H).to(torch.int32).clamp(0, H-1)\n",
    "        col = (x * W).to(torch.int32).clamp(0, W-1)\n",
    "        return row * W + col\n",
    "\n",
    "    def approximate_relative_distances(self, target_index, H, W, m):\n",
    "        \"\"\"Compute spatial bias\"\"\"\n",
    "        alpha = self.alpha\n",
    "        N = H * W\n",
    "        t = target_index.float() / N\n",
    "\n",
    "        token_positions = torch.tensor(\n",
    "            [(i + 0.5) / m for i in range(m)],\n",
    "            device=target_index.device\n",
    "        )\n",
    "\n",
    "        t_expanded = t.unsqueeze(0)\n",
    "        tokens_expanded = token_positions.unsqueeze(1)\n",
    "        rel_distances = -alpha * torch.abs(t_expanded - tokens_expanded)**2\n",
    "\n",
    "        return rel_distances\n",
    "\n",
    "    def forward(self, coords_decoding, tokens, coords_modulation=None):\n",
    "        \"\"\"\n",
    "        Forward pass with larger capacity\n",
    "\n",
    "        Args:\n",
    "            coords_decoding: (B, H, W, 2) - where to predict RGB\n",
    "            tokens: (B, L, D) - LP token features\n",
    "            coords_modulation: (B, H, W, 2) - where to extract modulation (None = test mode)\n",
    "        \"\"\"\n",
    "        B, query_shape = coords_decoding.shape[0], coords_decoding.shape[1:-1]\n",
    "        coords_dec = coords_decoding.view(B, -1, coords_decoding.shape[-1])\n",
    "\n",
    "        if coords_modulation is not None:\n",
    "            coords_mod = coords_modulation.view(B, -1, coords_modulation.shape[-1])\n",
    "        else:\n",
    "            coords_mod = coords_dec\n",
    "\n",
    "        # === MODULATION EXTRACTION ===\n",
    "\n",
    "        grid_mod = coords_mod[0]\n",
    "        indexes = self.get_patch_index(grid_mod, self.patch_num, self.patch_num)\n",
    "        rel_distances = self.approximate_relative_distances(\n",
    "            indexes, self.patch_num, self.patch_num, tokens.shape[1]\n",
    "        )\n",
    "        bias = einops.repeat(rel_distances, 'l n -> b l n', b=B)\n",
    "\n",
    "        # Query encoding\n",
    "        x_q = einops.repeat(\n",
    "            gaussian_fourier_encode(coords_mod[0], self.B_q), 'l d -> b l d', b=B\n",
    "        )\n",
    "        x_q = self.act(self.query_lin(x_q))\n",
    "\n",
    "        # Extract modulation\n",
    "        modulation_vector = self.modulation_ca(x_q, context=tokens, bias=bias)\n",
    "\n",
    "        # === MULTI-SCALE DECODING (with larger capacity) ===\n",
    "\n",
    "        modulations_l = []\n",
    "        for k in range(self.layer_num):\n",
    "            # Bandwidth encoding (DEEPER network)\n",
    "            x_l = einops.repeat(\n",
    "                gaussian_fourier_encode(coords_dec[0], self.B_ls[k]), 'l d -> b l d', b=B\n",
    "            )\n",
    "            h_l = self.bandwidth_lins[k](x_l)\n",
    "\n",
    "            # Modulation projection (DEEPER network)\n",
    "            m_proj = self.modulation_lins[k](modulation_vector)\n",
    "\n",
    "            # Combine\n",
    "            m_l = self.act(h_l + m_proj)\n",
    "            modulations_l.append(m_l)\n",
    "\n",
    "        # Residual connections (DEEPER network)\n",
    "        h_v = [modulations_l[0]]\n",
    "        for i in range(self.layer_num - 1):\n",
    "            h_vl = self.hv_lins[i](modulations_l[i+1] + h_v[i])\n",
    "            h_v.append(h_vl)\n",
    "\n",
    "        # Output layers (DEEPER network)\n",
    "        outs = [self.out_lins[i](h_v[i]) for i in range(self.layer_num)]\n",
    "        out = sum(outs)\n",
    "        out = out.view(B, *query_shape, -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"âœ“ LARGER LAINR Decoder defined\")\n",
    "print(\"  - hidden_dim: 512 (was 256)\")\n",
    "print(\"  - num_layers: 3 per scale (was 1)\")\n",
    "print(\"  - Expected parameters: ~4M (was ~450K)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Complete Model\n",
    "\n",
    "class MambaGINR_LargeDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 patch_size=2,\n",
    "                 in_channels=3,\n",
    "                 dim=256,\n",
    "                 num_lp_tokens=256,\n",
    "                 feature_dim=64,\n",
    "                 sigma_q=16,\n",
    "                 sigma_ls=[128, 32],\n",
    "                 hidden_dim=512,\n",
    "                 num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.patch_encoder = PatchEncoder(patch_size=patch_size, in_channels=in_channels, dim=dim)\n",
    "        self.lp_tokens = LearnablePositionTokens(num_tokens=num_lp_tokens, dim=dim)\n",
    "        self.mamba = BiMamba(d_model=dim)\n",
    "        \n",
    "        # Decoder (LARGER)\n",
    "        self.num_patches = (32 // patch_size) ** 2\n",
    "        self.hyponet = LAINRDecoderLarge(\n",
    "            feature_dim=feature_dim,\n",
    "            input_dim=2,\n",
    "            output_dim=3,\n",
    "            sigma_q=sigma_q,\n",
    "            sigma_ls=sigma_ls,\n",
    "            n_patches=self.num_patches,\n",
    "            hidden_dim=hidden_dim,\n",
    "            context_dim=dim,\n",
    "            learnable_frequencies=True,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        B = x.shape[0]\n",
    "        patch_features = self.patch_encoder(x)\n",
    "        lp_tokens = self.lp_tokens(B)\n",
    "        combined = torch.cat([patch_features, lp_tokens], dim=1)\n",
    "        features = self.mamba(combined)\n",
    "        lp_features = features[:, -lp_tokens.shape[1]:, :]\n",
    "        return lp_features\n",
    "\n",
    "    def forward(self, x, coords_decoding, coords_modulation=None):\n",
    "        lp_features = self.encode(x)\n",
    "        rgb = self.hyponet(coords_decoding, lp_features, coords_modulation)\n",
    "        return rgb\n",
    "\n",
    "print(\"âœ“ Complete model defined with LARGER decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Functions\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr=5e-4, warmup_epochs=5, max_epoch=40):\n",
    "    \"\"\"Learning rate schedule with warmup + cosine annealing\"\"\"\n",
    "    min_lr = 1e-8\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        t = (epoch - warmup_epochs) / (max_epoch - warmup_epochs)\n",
    "        lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * t))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train_epoch_correct(model, loader, optimizer, device, epoch,\n",
    "                        resolution=32,\n",
    "                        jitter_std=None,\n",
    "                        offset_std=0.0):\n",
    "    \"\"\"\n",
    "    Training with CORRECT jittering specification\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_psnr = 0\n",
    "\n",
    "    # Auto-compute jittering std if not provided\n",
    "    if jitter_std is None:\n",
    "        pixel_size = 1.0 / resolution\n",
    "        max_allowed_jitter = pixel_size / 2\n",
    "        jitter_std = max_allowed_jitter / 3  # 3-sigma rule\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Create base coordinate grid INSIDE loop (critical!)\n",
    "        base_coords = create_coordinate_grid(resolution, resolution, device)\n",
    "\n",
    "        # Apply jittering for modulation extraction\n",
    "        jitter_small = torch.randn_like(base_coords) * jitter_std\n",
    "        coords_modulation = (base_coords + jitter_small).clamp(0, 1)\n",
    "\n",
    "        # Prediction offset (default: 0)\n",
    "        if offset_std > 0:\n",
    "            prediction_offset = torch.randn_like(base_coords) * offset_std\n",
    "            coords_decoding = (coords_modulation + prediction_offset).clamp(0, 1)\n",
    "        else:\n",
    "            coords_decoding = coords_modulation\n",
    "\n",
    "        # Repeat for batch\n",
    "        coords_mod_batch = einops.repeat(coords_modulation, 'h w d -> b h w d', b=B)\n",
    "        coords_dec_batch = einops.repeat(coords_decoding, 'h w d -> b h w d', b=B)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(images, coords_dec_batch, coords_mod_batch)\n",
    "\n",
    "        # Ground truth\n",
    "        gt = einops.rearrange(images, 'b c h w -> b h w c')\n",
    "\n",
    "        # Loss\n",
    "        mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "        loss = mses.mean()\n",
    "        psnr = (-10 * torch.log10(mses)).mean()\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_psnr += psnr.item()\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'psnr': f\"{psnr.item():.2f}\",\n",
    "            'jitter': f\"Ïƒ={jitter_std:.5f}\"\n",
    "        })\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "\n",
    "def validate_correct(model, loader, device, resolution=32):\n",
    "    \"\"\"Validation with NO jittering\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_psnr = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            B = images.shape[0]\n",
    "\n",
    "            # Exact coordinates (no jittering)\n",
    "            coords = create_coordinate_grid(resolution, resolution, device)\n",
    "            coords_batch = einops.repeat(coords, 'h w d -> b h w d', b=B)\n",
    "\n",
    "            # Forward pass (coords_modulation=None â†’ test mode)\n",
    "            pred = model(images, coords_batch, coords_modulation=None)\n",
    "\n",
    "            # Ground truth\n",
    "            gt = einops.rearrange(images, 'b c h w -> b h w c')\n",
    "\n",
    "            # Metrics\n",
    "            mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "            loss = mses.mean()\n",
    "            psnr = (-10 * torch.log10(mses)).mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_psnr += psnr.item()\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "\n",
    "def super_resolve_correct(model, images, target_resolution=128, device='cpu'):\n",
    "    \"\"\"Super-resolution with NO jittering\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Encode at training resolution\n",
    "        lp_features = model.encode(images)\n",
    "\n",
    "        # Decode at target resolution (no jittering)\n",
    "        coords = create_coordinate_grid(target_resolution, target_resolution, device)\n",
    "        coords_batch = einops.repeat(coords, 'h w d -> b h w d', b=B)\n",
    "\n",
    "        # Predict (coords_modulation=None â†’ test mode)\n",
    "        pred = model.hyponet(coords_batch, lp_features, coords_modulation=None)\n",
    "\n",
    "        # Rearrange to image format\n",
    "        pred_images = einops.rearrange(pred, 'b h w c -> b c h w')\n",
    "\n",
    "    return pred_images\n",
    "\n",
    "print(\"âœ“ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data Loading\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Initialization\n",
    "\n",
    "model = MambaGINR_LargeDecoder(\n",
    "    patch_size=2,\n",
    "    in_channels=3,\n",
    "    dim=256,\n",
    "    num_lp_tokens=256,\n",
    "    feature_dim=64,\n",
    "    sigma_q=16,\n",
    "    sigma_ls=[128, 32],\n",
    "    hidden_dim=512,      # LARGER\n",
    "    num_layers=3         # DEEPER\n",
    ").to(device)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "decoder_params = count_parameters(model.hyponet)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Decoder parameters: {decoder_params:,}\")\n",
    "print(f\"\\nDecoder capacity: {decoder_params / 1e6:.1f}M params (was ~0.45M)\")\n",
    "print(f\"Expected improvement: +3-5 dB PSNR at 128Ã—128 super-resolution\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "print(\"\\nâœ“ Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "num_epochs = 40\n",
    "resolution = 32\n",
    "\n",
    "print(f\"\\nTraining LARGER decoder for {num_epochs} epochs\")\n",
    "print(f\"Resolution: {resolution}Ã—{resolution}\")\n",
    "print(f\"Auto jitter_std: {1/(6*resolution):.6f}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, base_lr=5e-4, max_epoch=num_epochs)\n",
    "    \n",
    "    train_loss, train_psnr = train_epoch_correct(\n",
    "        model, train_loader, optimizer, device,\n",
    "        epoch+1,\n",
    "        resolution=resolution,\n",
    "        jitter_std=None,    # Auto: 1/(6*32)\n",
    "        offset_std=0.0\n",
    "    )\n",
    "    \n",
    "    val_loss, val_psnr = validate_correct(\n",
    "        model, test_loader, device, resolution=resolution\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | LR: {lr:.6f}\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, PSNR: {val_psnr:.2f} dB\\n\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Super-Resolution Test\n",
    "\n",
    "# Get test images\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images[:8].to(device)\n",
    "\n",
    "# Super-resolve at multiple resolutions\n",
    "print(\"Testing super-resolution with LARGER decoder...\\n\")\n",
    "\n",
    "sr_64 = super_resolve_correct(model, test_images, target_resolution=64, device=device)\n",
    "sr_128 = super_resolve_correct(model, test_images, target_resolution=128, device=device)\n",
    "sr_256 = super_resolve_correct(model, test_images, target_resolution=256, device=device)\n",
    "\n",
    "print(f\"Original: {test_images.shape}\")\n",
    "print(f\"SR 64Ã—64: {sr_64.shape}\")\n",
    "print(f\"SR 128Ã—128: {sr_128.shape}\")\n",
    "print(f\"SR 256Ã—256: {sr_256.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original 32Ã—32\n",
    "    axes[0, i].imshow(test_images[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('32Ã—32', fontsize=10)\n",
    "    \n",
    "    # SR 64Ã—64\n",
    "    axes[1, i].imshow(sr_64[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('64Ã—64 SR', fontsize=10)\n",
    "    \n",
    "    # SR 128Ã—128\n",
    "    axes[2, i].imshow(sr_128[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_title('128Ã—128 SR', fontsize=10)\n",
    "    \n",
    "    # SR 256Ã—256\n",
    "    axes[3, i].imshow(sr_256[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_title('256Ã—256 SR', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('larger_decoder_superresolution_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Super-resolution visualization saved as 'larger_decoder_superresolution_results.png'\")\n",
    "print(\"\\nExpected improvements with LARGER decoder:\")\n",
    "print(\"  - Better texture synthesis\")\n",
    "print(\"  - More high-frequency details at 128Ã—128 and 256Ã—256\")\n",
    "print(\"  - Less smooth/blurry appearance\")\n",
    "print(\"  - Expected +3-5 dB PSNR improvement\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
