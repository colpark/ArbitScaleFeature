{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA-GINR with SCENT-Style Decoder (FIXED)\n",
    "\n",
    "## Fix Applied:\n",
    "- **Spatial bias dimension fix**: Now uses actual coordinate grid dimensions instead of fixed patch_num\n",
    "- This prevents RuntimeError: tensor size mismatch (256 vs 1024)\n",
    "\n",
    "## Key Changes from ResidualBlock Version:\n",
    "1. **Attention + FeedForward blocks** (not ResidualBlocks)\n",
    "2. **4x expansion** in FeedForward (512 → 2048 → 512)\n",
    "3. **Skip connections** from Fourier encoding to output\n",
    "4. **GEGLU gating**, LayerNorm, self-attention\n",
    "5. **Sinusoidal initialization** for LP tokens\n",
    "\n",
    "## Expected Improvement:\n",
    "- **+5-10 dB PSNR** at 128×128 super-resolution\n",
    "- Sharp high-frequency details instead of smooth blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log, pi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def gaussian_fourier_encode(coords, B_matrix):\n",
    "    if coords.dim() == 3:\n",
    "        coords = coords.view(-1, coords.shape[-1])\n",
    "    proj = 2 * np.pi * coords @ B_matrix.T\n",
    "    features = torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "    return features\n",
    "\n",
    "def create_coordinate_grid(H, W, device):\n",
    "    y = torch.linspace(0, 1, H, device=device)\n",
    "    x = torch.linspace(0, 1, W, device=device)\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    coords = torch.stack([yy, xx], dim=-1)\n",
    "    return coords\n",
    "\n",
    "def get_sinusoidal_embeddings(n, d):\n",
    "    assert d % 2 == 0\n",
    "    position = torch.arange(n, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d, 2).float() * -(log(10000.0) / d))\n",
    "    pe = torch.zeros(n, d)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: SCENT-Style Building Blocks\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, bias=None):\n",
    "        h = self.heads\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        \n",
    "        if exists(bias):\n",
    "            if bias.dim() == 3 and bias.shape[0] == x.shape[0]:\n",
    "                bias = repeat(bias, 'b l n -> (b h) l n', h=h)\n",
    "            sim = sim + bias\n",
    "        \n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "        \n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "print(\"✓ SCENT-style blocks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Encoder Components\n",
    "\n",
    "class BiMamba(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.forward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.backward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.proj = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_forward = self.forward_mamba(x)\n",
    "        x_backward = self.backward_mamba(torch.flip(x, dims=[1]))\n",
    "        x_backward = torch.flip(x_backward, dims=[1])\n",
    "        x = torch.cat([x_forward, x_backward], dim=-1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, patch_size=2, in_channels=3, dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_channels, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        p = self.patch_size\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LearnablePositionTokens(nn.Module):\n",
    "    def __init__(self, num_tokens=256, dim=256, use_sinusoidal=True):\n",
    "        super().__init__()\n",
    "        if use_sinusoidal:\n",
    "            init_tokens = get_sinusoidal_embeddings(num_tokens, dim)\n",
    "        else:\n",
    "            init_tokens = torch.randn(num_tokens, dim) * 0.02\n",
    "        self.tokens = nn.Parameter(init_tokens, requires_grad=True)\n",
    "\n",
    "    def forward(self, B):\n",
    "        return repeat(self.tokens, 'n d -> b n d', b=B)\n",
    "\n",
    "class LatentProcessor(nn.Module):\n",
    "    def __init__(self, dim, num_layers=2, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head)),\n",
    "                PreNorm(dim, FeedForward(dim, mult=4))\n",
    "            ])\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "print(\"✓ Encoder components defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SCENT-Style Decoder (FIXED)\n",
    "\n",
    "class LAINRDecoderSCENT(nn.Module):\n",
    "    def __init__(self, feature_dim=64, input_dim=2, output_dim=3,\n",
    "                 sigma_q=16, sigma_ls=[128, 32], n_patches=256,\n",
    "                 hidden_dim=512, context_dim=256,\n",
    "                 learnable_frequencies=True, num_layers=3,\n",
    "                 heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = len(sigma_ls)\n",
    "        self.n_features = feature_dim // 2\n",
    "        self.patch_num = int(math.sqrt(n_patches))\n",
    "        self.alpha = 10.0\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        B_q_init = torch.randn(self.n_features, input_dim) / sigma_q\n",
    "        B_ls_init = [torch.randn(self.n_features, input_dim) / sigma_ls[i]\n",
    "                     for i in range(self.layer_num)]\n",
    "\n",
    "        if learnable_frequencies:\n",
    "            self.B_q = nn.Parameter(B_q_init)\n",
    "            self.B_ls = nn.ParameterList([nn.Parameter(B_ls_init[i]) for i in range(self.layer_num)])\n",
    "        else:\n",
    "            self.register_buffer('B_q', B_q_init)\n",
    "            for i in range(self.layer_num):\n",
    "                self.register_buffer(f'B_l_{i}', B_ls_init[i])\n",
    "            self.B_ls = [getattr(self, f'B_l_{i}') for i in range(self.layer_num)]\n",
    "\n",
    "        self.query_lin = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.modulation_ca = PreNorm(hidden_dim, Attention(hidden_dim, context_dim, heads=2, dim_head=64), context_dim=context_dim)\n",
    "\n",
    "        self.bandwidth_lins = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(feature_dim, hidden_dim), nn.ReLU(),\n",
    "                *[nn.ModuleList([PreNorm(hidden_dim, Attention(hidden_dim, heads=heads, dim_head=dim_head)),\n",
    "                                 PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))])\n",
    "                  for _ in range(num_layers - 1)])\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        self.modulation_lins = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                *[nn.ModuleList([PreNorm(hidden_dim, Attention(hidden_dim, heads=heads, dim_head=dim_head)),\n",
    "                                 PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))])\n",
    "                  for _ in range(num_layers - 1)])\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        self.hv_lins = nn.ModuleList([\n",
    "            nn.ModuleList([PreNorm(hidden_dim, Attention(hidden_dim, heads=heads, dim_head=dim_head)),\n",
    "                          PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))])\n",
    "            for _ in range(self.layer_num - 1)\n",
    "        ])\n",
    "\n",
    "        self.fourier_skip_projs = nn.ModuleList([nn.Linear(feature_dim, hidden_dim) for _ in range(self.layer_num)])\n",
    "        self.out_lins = nn.ModuleList([nn.Linear(hidden_dim, output_dim) for _ in range(self.layer_num)])\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def get_patch_index(self, grid, H, W):\n",
    "        y, x = grid[:, 0], grid[:, 1]\n",
    "        row = (y * H).to(torch.int32).clamp(0, H-1)\n",
    "        col = (x * W).to(torch.int32).clamp(0, W-1)\n",
    "        return row * W + col\n",
    "\n",
    "    def approximate_relative_distances(self, target_index, H, W, m):\n",
    "        N = H * W\n",
    "        t = target_index.float() / N\n",
    "        token_positions = torch.tensor([(i + 0.5) / m for i in range(m)], device=target_index.device)\n",
    "        t_expanded = t.unsqueeze(0)\n",
    "        tokens_expanded = token_positions.unsqueeze(1)\n",
    "        return -self.alpha * torch.abs(t_expanded - tokens_expanded)**2\n",
    "\n",
    "    def apply_block_sequence(self, x, block_seq):\n",
    "        if isinstance(block_seq[0], nn.Linear):\n",
    "            x = block_seq[1](block_seq[0](x))\n",
    "            start_idx = 2\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        for i in range(start_idx, len(block_seq)):\n",
    "            block_list = block_seq[i]\n",
    "            if isinstance(block_list, nn.ModuleList):\n",
    "                attn, ff = block_list[0], block_list[1]\n",
    "                x = attn(x) + x\n",
    "                x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "    def forward(self, coords_decoding, tokens, coords_modulation=None):\n",
    "        B, query_shape = coords_decoding.shape[0], coords_decoding.shape[1:-1]\n",
    "        coords_dec = coords_decoding.view(B, -1, coords_decoding.shape[-1])\n",
    "        coords_mod = coords_modulation.view(B, -1, coords_modulation.shape[-1]) if coords_modulation is not None else coords_dec\n",
    "\n",
    "        # FIXED: Use actual coordinate grid dimensions\n",
    "        grid_mod = coords_mod[0]\n",
    "        num_queries = grid_mod.shape[0]\n",
    "        H_mod = W_mod = int(math.sqrt(num_queries))\n",
    "        indexes = self.get_patch_index(grid_mod, H_mod, W_mod)\n",
    "        rel_distances = self.approximate_relative_distances(indexes, H_mod, W_mod, tokens.shape[1])\n",
    "        bias = repeat(rel_distances, 'l n -> b l n', b=B)\n",
    "\n",
    "        x_q = repeat(gaussian_fourier_encode(coords_mod[0], self.B_q), 'l d -> b l d', b=B)\n",
    "        x_q = self.act(self.query_lin(x_q))\n",
    "        modulation_vector = self.modulation_ca(x_q, context=tokens, bias=bias)\n",
    "\n",
    "        modulations_l, fourier_encodings = [], []\n",
    "        for k in range(self.layer_num):\n",
    "            x_l_fourier = gaussian_fourier_encode(coords_dec[0], self.B_ls[k])\n",
    "            x_l_fourier_batch = repeat(x_l_fourier, 'l d -> b l d', b=B)\n",
    "            fourier_encodings.append(x_l_fourier_batch)\n",
    "            h_l = self.apply_block_sequence(x_l_fourier_batch, self.bandwidth_lins[k])\n",
    "            m_proj = self.apply_block_sequence(modulation_vector, self.modulation_lins[k])\n",
    "            modulations_l.append(self.act(h_l + m_proj))\n",
    "\n",
    "        h_v = [modulations_l[0]]\n",
    "        for i in range(self.layer_num - 1):\n",
    "            x_combined = modulations_l[i+1] + h_v[i]\n",
    "            attn, ff = self.hv_lins[i][0], self.hv_lins[i][1]\n",
    "            x_combined = attn(x_combined) + x_combined\n",
    "            x_combined = ff(x_combined) + x_combined\n",
    "            h_v.append(x_combined)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.layer_num):\n",
    "            fourier_skip = self.fourier_skip_projs[i](fourier_encodings[i])\n",
    "            outs.append(self.out_lins[i](h_v[i] + fourier_skip))\n",
    "\n",
    "        out = sum(outs).view(B, *query_shape, -1)\n",
    "        return out\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"✓ SCENT-style decoder defined (FIXED spatial bias)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Complete Model\n",
    "\n",
    "class MambaGINR_SCENT(nn.Module):\n",
    "    def __init__(self, patch_size=2, in_channels=3, dim=256, num_lp_tokens=256,\n",
    "                 feature_dim=64, sigma_q=16, sigma_ls=[128, 32],\n",
    "                 hidden_dim=512, num_layers=3, num_latent_layers=2):\n",
    "        super().__init__()\n",
    "        self.patch_encoder = PatchEncoder(patch_size=patch_size, in_channels=in_channels, dim=dim)\n",
    "        self.lp_tokens = LearnablePositionTokens(num_tokens=num_lp_tokens, dim=dim, use_sinusoidal=True)\n",
    "        self.mamba = BiMamba(d_model=dim)\n",
    "        self.latent_processor = LatentProcessor(dim=dim, num_layers=num_latent_layers, heads=8, dim_head=64)\n",
    "        self.num_patches = (32 // patch_size) ** 2\n",
    "        self.hyponet = LAINRDecoderSCENT(\n",
    "            feature_dim=feature_dim, input_dim=2, output_dim=3,\n",
    "            sigma_q=sigma_q, sigma_ls=sigma_ls, n_patches=self.num_patches,\n",
    "            hidden_dim=hidden_dim, context_dim=dim,\n",
    "            learnable_frequencies=True, num_layers=num_layers, heads=8, dim_head=64\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        B = x.shape[0]\n",
    "        patch_features = self.patch_encoder(x)\n",
    "        lp_tokens = self.lp_tokens(B)\n",
    "        combined = torch.cat([patch_features, lp_tokens], dim=1)\n",
    "        features = self.mamba(combined)\n",
    "        features = self.latent_processor(features)\n",
    "        return features[:, -lp_tokens.shape[1]:, :]\n",
    "\n",
    "    def forward(self, x, coords_decoding, coords_modulation=None):\n",
    "        lp_features = self.encode(x)\n",
    "        return self.hyponet(coords_decoding, lp_features, coords_modulation)\n",
    "\n",
    "print(\"✓ Complete model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Functions\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr=5e-4, warmup_epochs=5, max_epoch=40):\n",
    "    min_lr = 1e-8\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        t = (epoch - warmup_epochs) / (max_epoch - warmup_epochs)\n",
    "        lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * t))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, epoch, resolution=32):\n",
    "    model.train()\n",
    "    total_loss, total_psnr = 0, 0\n",
    "    jitter_std = (1.0 / resolution) / 6\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "        base_coords = create_coordinate_grid(resolution, resolution, device)\n",
    "        jitter = torch.randn_like(base_coords) * jitter_std\n",
    "        coords = (base_coords + jitter).clamp(0, 1)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "\n",
    "        pred = model(images, coords_batch, coords_batch)\n",
    "        gt = rearrange(images, 'b c h w -> b h w c')\n",
    "        mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "        loss = mses.mean()\n",
    "        psnr = (-10 * torch.log10(mses)).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_psnr += psnr.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'psnr': f\"{psnr.item():.2f}\"})\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "def validate(model, loader, device, resolution=32):\n",
    "    model.eval()\n",
    "    total_loss, total_psnr = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            B = images.shape[0]\n",
    "            coords = create_coordinate_grid(resolution, resolution, device)\n",
    "            coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "            pred = model(images, coords_batch, None)\n",
    "            gt = rearrange(images, 'b c h w -> b h w c')\n",
    "            mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "            total_loss += mses.mean().item()\n",
    "            total_psnr += (-10 * torch.log10(mses)).mean().item()\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "def super_resolve(model, images, target_resolution=128, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B = images.shape[0]\n",
    "        lp_features = model.encode(images)\n",
    "        coords = create_coordinate_grid(target_resolution, target_resolution, device)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "        pred = model.hyponet(coords_batch, lp_features, None)\n",
    "        return rearrange(pred, 'b h w c -> b c h w')\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data Loading\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Initialization\n",
    "\n",
    "model = MambaGINR_SCENT(\n",
    "    patch_size=2, in_channels=3, dim=256, num_lp_tokens=256,\n",
    "    feature_dim=64, sigma_q=16, sigma_ls=[128, 32],\n",
    "    hidden_dim=512, num_layers=3, num_latent_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Total parameters: {count_parameters(model):,}\")\n",
    "print(f\"Decoder parameters: {count_parameters(model.hyponet):,}\")\n",
    "print(f\"Expected: +5-10 dB PSNR at 128×128\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "num_epochs = 40\n",
    "best_val_psnr = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, base_lr=5e-4, max_epoch=num_epochs)\n",
    "    train_loss, train_psnr = train_epoch(model, train_loader, optimizer, device, epoch+1)\n",
    "    val_loss, val_psnr = validate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} | LR: {lr:.6f}\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
    "    \n",
    "    if val_psnr > best_val_psnr:\n",
    "        best_val_psnr = val_psnr\n",
    "        torch.save(model.state_dict(), 'mamba_ginr_scent_best.pth')\n",
    "        print(f\"  → Best saved (PSNR: {best_val_psnr:.2f} dB)\")\n",
    "\n",
    "print(f\"\\nBest validation PSNR: {best_val_psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Super-Resolution Test\n",
    "\n",
    "model.load_state_dict(torch.load('mamba_ginr_scent_best.pth'))\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images[:8].to(device)\n",
    "\n",
    "sr_64 = super_resolve(model, test_images, 64, device)\n",
    "sr_128 = super_resolve(model, test_images, 128, device)\n",
    "sr_256 = super_resolve(model, test_images, 256, device)\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(test_images[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].imshow(sr_64[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2, i].imshow(sr_128[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[3, i].imshow(sr_256[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    for j in range(4):\n",
    "        axes[j, i].axis('off')\n",
    "\n",
    "labels = ['32×32', '64×64 SR', '128×128 SR', '256×256 SR']\n",
    "for j, label in enumerate(labels):\n",
    "    axes[j, 0].set_ylabel(label, fontsize=12, rotation=0, labelpad=30)\n",
    "\n",
    "plt.suptitle('SCENT-Style Decoder: Super-Resolution Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('scent_sr_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Results saved as 'scent_sr_results.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
