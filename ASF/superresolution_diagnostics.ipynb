{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Resolution Diagnostics for MAMBA-GINR\n",
    "\n",
    "This notebook diagnoses WHY super-resolution might be sub-optimal.\n",
    "\n",
    "**Key insight**: True super-resolution CAN work at single training resolution through:\n",
    "1. High-frequency Fourier features (σ_l = [128, 32])\n",
    "2. MLP learning to generate textures from semantic features\n",
    "3. Proper jittering for continuous coordinate learning\n",
    "\n",
    "We'll test which component is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft2, fftshift\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you've already trained the model and loaded it\n",
    "# model = ... (your trained MAMBA-GINR model)\n",
    "# test_images = ... (test images)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic 1: Fourier Feature Activation Analysis\n",
    "\n",
    "Check if the network is actually USING the high-frequency Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_fourier_feature_usage(model, test_image, resolutions=[32, 64, 128]):\n",
    "    \"\"\"\n",
    "    Analyze which Fourier frequencies are being utilized by the network\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    img = test_image[:1].to(device)\n",
    "    lp_features = model.encode(img)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(resolutions), figsize=(15, 8))\n",
    "    \n",
    "    for idx, res in enumerate(resolutions):\n",
    "        # Create coordinate grid\n",
    "        coord = create_coordinate_grid(res, res, device).unsqueeze(0)\n",
    "        \n",
    "        # Hook to capture Fourier features\n",
    "        fourier_features_list = []\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            # Capture activations from bandwidth encoding\n",
    "            fourier_features_list.append(input[0].detach())\n",
    "        \n",
    "        # Register hooks on bandwidth_lins layers\n",
    "        hooks = []\n",
    "        for layer in model.hyponet.bandwidth_lins:\n",
    "            hooks.append(layer.register_forward_hook(hook_fn))\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(lp_features, coord)\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Analyze frequency usage for each scale\n",
    "        for scale_idx, fourier_feats in enumerate(fourier_features_list):\n",
    "            # fourier_feats: (B, HW, feature_dim)\n",
    "            # Compute activation magnitude per feature dimension\n",
    "            activation_magnitude = fourier_feats[0].abs().mean(dim=0).cpu().numpy()\n",
    "            \n",
    "            # Plot activation vs frequency\n",
    "            ax = axes[scale_idx, idx]\n",
    "            \n",
    "            # Get corresponding omegas\n",
    "            sigma = [128, 32][scale_idx]\n",
    "            n = len(activation_magnitude) // 4  # 2 coords * 2 (sin/cos)\n",
    "            omegas = torch.logspace(1, np.log10(sigma), n).numpy()\n",
    "            \n",
    "            # Plot for each coordinate's sin/cos pairs\n",
    "            ax.plot(omegas, activation_magnitude[:n], label='X-sin', alpha=0.7)\n",
    "            ax.plot(omegas, activation_magnitude[n:2*n], label='X-cos', alpha=0.7)\n",
    "            ax.plot(omegas, activation_magnitude[2*n:3*n], label='Y-sin', alpha=0.7)\n",
    "            ax.plot(omegas, activation_magnitude[3*n:4*n], label='Y-cos', alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Frequency ω')\n",
    "            ax.set_ylabel('Activation Magnitude')\n",
    "            ax.set_title(f'{res}×{res}, σ={sigma}')\n",
    "            ax.set_xscale('log')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            if idx == 0:\n",
    "                ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fourier_feature_activation.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FOURIER FEATURE ACTIVATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nWhat to look for:\")\n",
    "    print(\"  ✓ High activations at HIGH frequencies (ω > 50) → Network uses high-freq\")\n",
    "    print(\"  ✗ Low activations at high frequencies → Network ignores them (resampling)\")\n",
    "    print(\"\\nIf activations drop off before max frequency (σ), the network is NOT\")\n",
    "    print(\"utilizing the full high-frequency capacity!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Run analysis\n",
    "analyze_fourier_feature_usage(model, test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic 2: Gradient Magnitude Analysis\n",
    "\n",
    "Compare spatial gradients in super-resolved images vs bicubic upsampling.\n",
    "Sharp edges = high gradients = true super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_gradient_magnitude(img):\n",
    "    \"\"\"\n",
    "    Compute spatial gradient magnitude\n",
    "    img: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    # Sobel filters\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                           dtype=torch.float32, device=img.device).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                           dtype=torch.float32, device=img.device).view(1, 1, 3, 3)\n",
    "    \n",
    "    # Apply to each channel\n",
    "    grad_x = F.conv2d(img, sobel_x.repeat(img.shape[1], 1, 1, 1), \n",
    "                      padding=1, groups=img.shape[1])\n",
    "    grad_y = F.conv2d(img, sobel_y.repeat(img.shape[1], 1, 1, 1), \n",
    "                      padding=1, groups=img.shape[1])\n",
    "    \n",
    "    # Magnitude\n",
    "    grad_mag = torch.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    return grad_mag\n",
    "\n",
    "\n",
    "def analyze_gradient_sharpness(model, test_images, device, num_samples=8):\n",
    "    \"\"\"\n",
    "    Compare gradient distributions: model vs bicubic\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    gradients_32_model = []\n",
    "    gradients_64_model = []\n",
    "    gradients_128_model = []\n",
    "    gradients_64_bicubic = []\n",
    "    gradients_128_bicubic = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img = test_images[i:i+1].to(device)\n",
    "            \n",
    "            # Model reconstructions\n",
    "            coord_32 = create_coordinate_grid(32, 32, device).unsqueeze(0)\n",
    "            recon_32 = model(img, coord_32)\n",
    "            recon_32 = einops.rearrange(recon_32, 'b h w c -> b c h w')\n",
    "            \n",
    "            coord_64 = create_coordinate_grid(64, 64, device).unsqueeze(0)\n",
    "            recon_64 = model(img, coord_64)\n",
    "            recon_64 = einops.rearrange(recon_64, 'b h w c -> b c h w')\n",
    "            \n",
    "            coord_128 = create_coordinate_grid(128, 128, device).unsqueeze(0)\n",
    "            recon_128 = model(img, coord_128)\n",
    "            recon_128 = einops.rearrange(recon_128, 'b h w c -> b c h w')\n",
    "            \n",
    "            # Bicubic upsampling\n",
    "            bicubic_64 = F.interpolate(img, size=64, mode='bicubic', align_corners=False)\n",
    "            bicubic_128 = F.interpolate(img, size=128, mode='bicubic', align_corners=False)\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradients_32_model.append(compute_gradient_magnitude(recon_32).flatten())\n",
    "            gradients_64_model.append(compute_gradient_magnitude(recon_64).flatten())\n",
    "            gradients_128_model.append(compute_gradient_magnitude(recon_128).flatten())\n",
    "            gradients_64_bicubic.append(compute_gradient_magnitude(bicubic_64).flatten())\n",
    "            gradients_128_bicubic.append(compute_gradient_magnitude(bicubic_128).flatten())\n",
    "    \n",
    "    # Concatenate all samples\n",
    "    grad_32_model = torch.cat(gradients_32_model).cpu().numpy()\n",
    "    grad_64_model = torch.cat(gradients_64_model).cpu().numpy()\n",
    "    grad_128_model = torch.cat(gradients_128_model).cpu().numpy()\n",
    "    grad_64_bicubic = torch.cat(gradients_64_bicubic).cpu().numpy()\n",
    "    grad_128_bicubic = torch.cat(gradients_128_bicubic).cpu().numpy()\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 32×32 (reference)\n",
    "    axes[0].hist(grad_32_model, bins=100, alpha=0.7, density=True, label='32×32')\n",
    "    axes[0].set_xlabel('Gradient Magnitude')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('32×32 (Training Resolution)')\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # 64×64\n",
    "    axes[1].hist(grad_64_bicubic, bins=100, alpha=0.5, density=True, \n",
    "                label='Bicubic', color='gray')\n",
    "    axes[1].hist(grad_64_model, bins=100, alpha=0.7, density=True, \n",
    "                label='MAMBA-GINR', color='steelblue')\n",
    "    axes[1].set_xlabel('Gradient Magnitude')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].set_title('64×64 Super-Resolution')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 128×128\n",
    "    axes[2].hist(grad_128_bicubic, bins=100, alpha=0.5, density=True, \n",
    "                label='Bicubic', color='gray')\n",
    "    axes[2].hist(grad_128_model, bins=100, alpha=0.7, density=True, \n",
    "                label='MAMBA-GINR', color='steelblue')\n",
    "    axes[2].set_xlabel('Gradient Magnitude')\n",
    "    axes[2].set_ylabel('Density')\n",
    "    axes[2].set_title('128×128 Super-Resolution')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gradient_sharpness_analysis.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GRADIENT MAGNITUDE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    def compute_stats(grads, name):\n",
    "        mean = grads.mean()\n",
    "        median = np.median(grads)\n",
    "        p95 = np.percentile(grads, 95)\n",
    "        p99 = np.percentile(grads, 99)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean:   {mean:.4f}\")\n",
    "        print(f\"  Median: {median:.4f}\")\n",
    "        print(f\"  95th percentile: {p95:.4f}\")\n",
    "        print(f\"  99th percentile: {p99:.4f}\")\n",
    "        return mean, p95, p99\n",
    "    \n",
    "    stats_32 = compute_stats(grad_32_model, \"32×32 (reference)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    stats_64_model = compute_stats(grad_64_model, \"64×64 MAMBA-GINR\")\n",
    "    stats_64_bicubic = compute_stats(grad_64_bicubic, \"64×64 Bicubic\")\n",
    "    \n",
    "    ratio_64_mean = stats_64_model[0] / (stats_64_bicubic[0] + 1e-10)\n",
    "    ratio_64_p95 = stats_64_model[1] / (stats_64_bicubic[1] + 1e-10)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    stats_128_model = compute_stats(grad_128_model, \"128×128 MAMBA-GINR\")\n",
    "    stats_128_bicubic = compute_stats(grad_128_bicubic, \"128×128 Bicubic\")\n",
    "    \n",
    "    ratio_128_mean = stats_128_model[0] / (stats_128_bicubic[0] + 1e-10)\n",
    "    ratio_128_p95 = stats_128_model[1] / (stats_128_bicubic[1] + 1e-10)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SHARPNESS RATIO (Model / Bicubic):\")\n",
    "    print(f\"  64×64  - Mean gradient: {ratio_64_mean:.3f}x\")\n",
    "    print(f\"  64×64  - 95th %ile:     {ratio_64_p95:.3f}x\")\n",
    "    print(f\"  128×128 - Mean gradient: {ratio_128_mean:.3f}x\")\n",
    "    print(f\"  128×128 - 95th %ile:     {ratio_128_p95:.3f}x\")\n",
    "    \n",
    "    print(\"\\n📊 INTERPRETATION:\")\n",
    "    if ratio_128_mean > 1.5 and ratio_128_p95 > 1.5:\n",
    "        print(\"  ✅ TRUE SUPER-RESOLUTION: Model generates sharper edges than bicubic\")\n",
    "    elif ratio_128_mean > 1.1:\n",
    "        print(\"  ⚠️  MARGINAL: Model slightly sharper, but limited\")\n",
    "    else:\n",
    "        print(\"  ❌ RESAMPLING: Model similar to bicubic (smooth interpolation)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return {\n",
    "        'ratio_64_mean': ratio_64_mean,\n",
    "        'ratio_128_mean': ratio_128_mean\n",
    "    }\n",
    "\n",
    "# Run analysis\n",
    "gradient_analysis = analyze_gradient_sharpness(model, test_images, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic 3: Modulation Vector Analysis\n",
    "\n",
    "Check if modulation vectors are diverse or just doing nearest-neighbor lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_modulation_diversity(model, test_image, resolution=128):\n",
    "    \"\"\"\n",
    "    Analyze spatial variation in modulation vectors\n",
    "    High diversity = continuous learning\n",
    "    Low diversity = discrete lookup table\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    img = test_image[:1].to(device)\n",
    "    lp_features = model.encode(img)\n",
    "    \n",
    "    # Create coordinate grid\n",
    "    coord = create_coordinate_grid(resolution, resolution, device).unsqueeze(0)\n",
    "    \n",
    "    # Hook to capture modulation vectors\n",
    "    modulation_vectors = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        modulation_vectors.append(output.detach())\n",
    "    \n",
    "    # Register hook on modulation cross-attention\n",
    "    hook = model.hyponet.modulation_ca.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model.decode(lp_features, coord)\n",
    "    \n",
    "    hook.remove()\n",
    "    \n",
    "    # Get modulation vectors: (B, HW, D)\n",
    "    mod_vec = modulation_vectors[0][0].cpu().numpy()  # (HW, D)\n",
    "    mod_vec = mod_vec.reshape(resolution, resolution, -1)  # (H, W, D)\n",
    "    \n",
    "    # Compute spatial gradients of modulation vectors\n",
    "    grad_y = np.abs(mod_vec[1:, :, :] - mod_vec[:-1, :, :])  # (H-1, W, D)\n",
    "    grad_x = np.abs(mod_vec[:, 1:, :] - mod_vec[:, :-1, :])  # (H, W-1, D)\n",
    "    \n",
    "    # Average gradient across feature dimensions\n",
    "    grad_y_mag = grad_y.mean(axis=-1)  # (H-1, W)\n",
    "    grad_x_mag = grad_x.mean(axis=-1)  # (H, W-1)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # First 3 modulation channels\n",
    "    for i in range(3):\n",
    "        im = axes[0, i].imshow(mod_vec[:, :, i], cmap='viridis')\n",
    "        axes[0, i].set_title(f'Modulation Channel {i}')\n",
    "        axes[0, i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[0, i])\n",
    "    \n",
    "    # Gradients\n",
    "    im1 = axes[1, 0].imshow(grad_y_mag, cmap='hot')\n",
    "    axes[1, 0].set_title('Modulation Gradient (Y)')\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1, 0])\n",
    "    \n",
    "    im2 = axes[1, 1].imshow(grad_x_mag, cmap='hot')\n",
    "    axes[1, 1].set_title('Modulation Gradient (X)')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[1, 1])\n",
    "    \n",
    "    # Gradient histogram\n",
    "    axes[1, 2].hist(grad_y_mag.flatten(), bins=100, alpha=0.5, label='Y-grad')\n",
    "    axes[1, 2].hist(grad_x_mag.flatten(), bins=100, alpha=0.5, label='X-grad')\n",
    "    axes[1, 2].set_xlabel('Modulation Gradient Magnitude')\n",
    "    axes[1, 2].set_ylabel('Frequency')\n",
    "    axes[1, 2].set_title('Gradient Distribution')\n",
    "    axes[1, 2].set_yscale('log')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('modulation_diversity_analysis.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    mean_grad = (grad_y_mag.mean() + grad_x_mag.mean()) / 2\n",
    "    std_grad = (grad_y_mag.std() + grad_x_mag.std()) / 2\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODULATION VECTOR DIVERSITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nMean modulation gradient: {mean_grad:.6f}\")\n",
    "    print(f\"Std modulation gradient:  {std_grad:.6f}\")\n",
    "    \n",
    "    print(\"\\n📊 INTERPRETATION:\")\n",
    "    if mean_grad > 0.01:\n",
    "        print(\"  ✅ HIGH DIVERSITY: Modulation vectors vary smoothly\")\n",
    "        print(\"     → Network learned continuous coordinate mapping\")\n",
    "    else:\n",
    "        print(\"  ❌ LOW DIVERSITY: Modulation vectors are blocky/constant\")\n",
    "        print(\"     → Network doing discrete lookup (need better jittering!)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return mean_grad\n",
    "\n",
    "# Run analysis\n",
    "modulation_diversity = analyze_modulation_diversity(model, test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic 4: Texture Synthesis Test\n",
    "\n",
    "Test if the network generates NEW texture patterns or just smooths existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def texture_synthesis_test(model, test_images, device, num_samples=4):\n",
    "    \"\"\"\n",
    "    Visual comparison of texture detail at different resolutions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(15, 12))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img = test_images[i:i+1].to(device)\n",
    "            \n",
    "            # Original 32×32\n",
    "            axes[i, 0].imshow(img[0].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "            axes[i, 0].set_title('32×32 Original' if i == 0 else '')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Model 64×64\n",
    "            coord_64 = create_coordinate_grid(64, 64, device).unsqueeze(0)\n",
    "            recon_64 = model(img, coord_64)[0].cpu().clamp(0, 1)\n",
    "            axes[i, 1].imshow(recon_64)\n",
    "            axes[i, 1].set_title('64×64 Model' if i == 0 else '')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Bicubic 64×64\n",
    "            bicubic_64 = F.interpolate(img, 64, mode='bicubic', align_corners=False)\n",
    "            axes[i, 2].imshow(bicubic_64[0].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "            axes[i, 2].set_title('64×64 Bicubic' if i == 0 else '')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            # Model 128×128 (crop center for detail)\n",
    "            coord_128 = create_coordinate_grid(128, 128, device).unsqueeze(0)\n",
    "            recon_128 = model(img, coord_128)[0].cpu().clamp(0, 1)\n",
    "            crop_128_model = recon_128[32:96, 32:96, :]  # Center 64×64\n",
    "            axes[i, 3].imshow(crop_128_model)\n",
    "            axes[i, 3].set_title('128×128 Model (crop)' if i == 0 else '')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "            # Bicubic 128×128 (crop center)\n",
    "            bicubic_128 = F.interpolate(img, 128, mode='bicubic', align_corners=False)\n",
    "            crop_128_bicubic = bicubic_128[0, :, 32:96, 32:96].cpu().permute(1, 2, 0).clamp(0, 1)\n",
    "            axes[i, 4].imshow(crop_128_bicubic)\n",
    "            axes[i, 4].set_title('128×128 Bicubic (crop)' if i == 0 else '')\n",
    "            axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('texture_synthesis_comparison.png', dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEXTURE SYNTHESIS TEST\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nVisual inspection:\")\n",
    "    print(\"  • Look at 128×128 crops (columns 4 vs 5)\")\n",
    "    print(\"  • TRUE SR: Model shows texture details not in bicubic\")\n",
    "    print(\"  • RESAMPLING: Model looks similar to bicubic (smooth)\")\n",
    "    print(\"\\nKey areas to check:\")\n",
    "    print(\"  - Edges: Sharp or blurry?\")\n",
    "    print(\"  - Textures: Repeated patterns or smooth?\")\n",
    "    print(\"  - Fine details: Visible or missing?\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Run test\n",
    "texture_synthesis_test(model, test_images, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic 5: Fix Jittering and Retrain (Quick Test)\n",
    "\n",
    "Test if fixing jittering improves super-resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement corrected jittering training\n",
    "# Compare before/after fixing jittering bug\n",
    "\n",
    "def train_epoch_corrected_jittering(model, loader, optimizer, device, epoch, \n",
    "                                    use_jittering=True, jitter_std=0.01):\n",
    "    \"\"\"\n",
    "    CORRECTED: Jittering happens INSIDE batch loop\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_psnr = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "        \n",
    "        # Create coordinate grid INSIDE loop (CORRECTED!)\n",
    "        coord = create_coordinate_grid(32, 32, device)\n",
    "        \n",
    "        # Jitter coordinates (different for each batch!)\n",
    "        if use_jittering:\n",
    "            coord = add_gaussian_noise_to_grid(coord, std=jitter_std)\n",
    "        \n",
    "        coord = einops.repeat(coord, 'h w d -> b h w d', b=B)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(images, coord)\n",
    "        gt = einops.rearrange(images, 'b c h w -> b h w c')\n",
    "        \n",
    "        # Loss\n",
    "        mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "        loss = mses.mean()\n",
    "        psnr = (-10 * torch.log10(mses)).mean()\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_psnr += psnr.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'psnr': f\"{psnr.item():.2f}\"})\n",
    "    \n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "print(\"Corrected training function defined.\")\n",
    "print(\"To test: Train for a few epochs and re-run diagnostics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Diagnostic Checklist\n",
    "\n",
    "Run all diagnostics above and check:\n",
    "\n",
    "### ✅ Signs of TRUE Super-Resolution:\n",
    "1. **Fourier features**: High activations at ω > 50\n",
    "2. **Gradients**: 1.5-2x sharper than bicubic\n",
    "3. **Modulation diversity**: Smooth spatial variation (grad > 0.01)\n",
    "4. **Textures**: Visible details not in bicubic\n",
    "\n",
    "### ❌ Signs of RESAMPLING:\n",
    "1. **Fourier features**: Low activations at high frequencies\n",
    "2. **Gradients**: Similar to bicubic (ratio ≈ 1.0)\n",
    "3. **Modulation diversity**: Blocky/constant (grad < 0.001)\n",
    "4. **Textures**: Smooth like bicubic\n",
    "\n",
    "### 🔧 If Resampling, Try:\n",
    "1. Fix jittering (move inside batch loop)\n",
    "2. Train longer (more epochs)\n",
    "3. Increase MLP capacity (hidden_dim = 512)\n",
    "4. Stronger jittering (std = 0.02)\n",
    "5. Add texture loss / perceptual loss\n",
    "\n",
    "The network SHOULD be capable of true super-resolution with proper training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
