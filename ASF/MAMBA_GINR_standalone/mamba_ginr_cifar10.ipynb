{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA-GINR on CIFAR-10\n",
    "\n",
    "## Original MAMBA-GINR Architecture\n",
    "\n",
    "This notebook implements the exact MAMBA-GINR architecture as described in the original paper.\n",
    "\n",
    "### Architecture:\n",
    "1. **Patch Encoder**: Convert 32×32 image to 16×16 patches (2×2 patch size)\n",
    "2. **Learnable Position Tokens**: 256 LP tokens with equidistant insertion\n",
    "3. **BiMamba Encoder**: 6 layers of bidirectional Mamba\n",
    "4. **LAINR Decoder**: Hyponetwork for arbitrary-resolution decoding\n",
    "\n",
    "### Target: 60 PSNR on CIFAR-10 (32×32 reconstruction)\n",
    "\n",
    "### Key Implementation Details:\n",
    "- dim=256, num_lp=256, mamba_depth=6\n",
    "- Fourier features for positional encoding\n",
    "- ResidualBlocks in decoder (original LAINR)\n",
    "- AdamW optimizer with cosine annealing\n",
    "- Batch size=64, lr=5e-4, 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.modules.block import Block\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def fourier_encode(coords, n_features=32, std=10.0):\n",
    "    \"\"\"\n",
    "    Fourier feature encoding for coordinates\n",
    "    Args:\n",
    "        coords: (N, 2) coordinates in [0, 1]\n",
    "        n_features: number of frequency components\n",
    "        std: standard deviation of frequency distribution\n",
    "    Returns:\n",
    "        features: (N, 2*n_features) [cos, sin]\n",
    "    \"\"\"\n",
    "    B = torch.randn(n_features, 2, device=coords.device) * std\n",
    "    proj = 2 * np.pi * coords @ B.T\n",
    "    return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "\n",
    "def create_coordinate_grid(H, W, device='cpu'):\n",
    "    \"\"\"Create normalized coordinate grid in [0,1]\"\"\"\n",
    "    y = torch.linspace(0, 1, H, device=device)\n",
    "    x = torch.linspace(0, 1, W, device=device)\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    return torch.stack([yy, xx], dim=-1)  # (H, W, 2)\n",
    "\n",
    "def get_sinusoidal_embeddings(n, d):\n",
    "    \"\"\"Sinusoidal positional embeddings\"\"\"\n",
    "    assert d % 2 == 0\n",
    "    position = torch.arange(n, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d, 2).float() * -(math.log(10000.0) / d))\n",
    "    pe = torch.zeros(n, d)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: BiMamba\n",
    "\n",
    "class BiMamba(nn.Module):\n",
    "    \"\"\"Bidirectional Mamba from MAMBA-GINR\"\"\"\n",
    "    def __init__(self, d_model=256, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.f_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.r_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.proj = nn.Linear(2 * d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, inference_params=None):\n",
    "        x_forward = self.f_mamba(x, inference_params=inference_params)\n",
    "        x_backward = self.r_mamba(torch.flip(x, dims=[1]), inference_params=inference_params)\n",
    "        x_backward = torch.flip(x_backward, dims=[1])\n",
    "        x = torch.cat([x_forward, x_backward], dim=-1)\n",
    "        return self.proj(x)\n",
    "\n",
    "print(\"✓ BiMamba defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Mamba Encoder\n",
    "\n",
    "class MambaEncoder(nn.Module):\n",
    "    \"\"\"Stack of Mamba blocks\"\"\"\n",
    "    def __init__(self, depth=6, dim=256, ff_dim=1024, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim,\n",
    "                mixer_cls=lambda d: BiMamba(d_model=d),\n",
    "                mlp_cls=lambda d: nn.Sequential(\n",
    "                    nn.Linear(d, ff_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(ff_dim, d),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                norm_cls=nn.LayerNorm,\n",
    "                fused_add_norm=False\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = None\n",
    "        for block in self.blocks:\n",
    "            x, residual = block(x, residual=residual, inference_params=None)\n",
    "        return x\n",
    "\n",
    "print(\"✓ MambaEncoder defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Learnable Position Tokens\n",
    "\n",
    "class LearnablePositionTokens(nn.Module):\n",
    "    \"\"\"LP tokens with sinusoidal initialization and equidistant placement\"\"\"\n",
    "    def __init__(self, num_tokens=256, dim=256, input_len=256):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.dim = dim\n",
    "        self.input_len = input_len\n",
    "        \n",
    "        # Initialize with sinusoidal embeddings\n",
    "        init_tokens = get_sinusoidal_embeddings(num_tokens, dim)\n",
    "        self.tokens = nn.Parameter(init_tokens, requires_grad=True)\n",
    "        \n",
    "        # Compute equidistant placement\n",
    "        total_len = input_len + num_tokens\n",
    "        self.lp_idxs = torch.linspace(0, total_len - 1, steps=num_tokens).long()\n",
    "        \n",
    "        # Compute interleave permutation\n",
    "        perm = torch.full((total_len,), -1, dtype=torch.long)\n",
    "        perm[self.lp_idxs] = torch.arange(input_len, input_len + num_tokens)\n",
    "        perm[perm == -1] = torch.arange(input_len)\n",
    "        self.register_buffer('perm', perm)\n",
    "    \n",
    "    def add_lp(self, x):\n",
    "        \"\"\"Add LP tokens to input sequence\"\"\"\n",
    "        B = x.shape[0]\n",
    "        lps = repeat(self.tokens, 'n d -> b n d', b=B)\n",
    "        x_full = torch.cat([x, lps], dim=1)  # (B, L+num_tokens, D)\n",
    "        return x_full[:, self.perm]  # Interleave\n",
    "    \n",
    "    def extract_lp(self, x):\n",
    "        \"\"\"Extract LP tokens from encoded sequence\"\"\"\n",
    "        return x[:, self.lp_idxs]\n",
    "\n",
    "print(\"✓ LearnablePositionTokens defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Original LAINR Decoder (ResidualBlock-based)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Simple residual block from original LAINR\"\"\"\n",
    "    def __init__(self, dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.net(x)\n",
    "\n",
    "\n",
    "class LAINRDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Original LAINR-style decoder with ResidualBlocks\n",
    "    \n",
    "    Architecture:\n",
    "    - Fourier encoding of coordinates\n",
    "    - Cross-attention to LP tokens\n",
    "    - ResidualBlocks for processing\n",
    "    - Multi-scale outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=32, input_dim=2, output_dim=3,\n",
    "                 hidden_dim=512, context_dim=256, n_patches=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.patch_num = int(math.sqrt(n_patches))\n",
    "        self.alpha = 10.0  # Spatial bias coefficient\n",
    "        \n",
    "        # Fourier encoding frequencies\n",
    "        self.register_buffer('B', torch.randn(n_features, input_dim) * 10.0)\n",
    "        feature_dim = 2 * n_features\n",
    "        \n",
    "        # Query encoding\n",
    "        self.query_proj = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        # Cross-attention for modulation extraction\n",
    "        self.to_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.to_kv = nn.Linear(context_dim, hidden_dim * 2)\n",
    "        self.attn_out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = (hidden_dim // 2) ** -0.5\n",
    "        \n",
    "        # Decoder processing\n",
    "        self.decoder_blocks = nn.Sequential(\n",
    "            nn.Linear(feature_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            ResidualBlock(hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def get_patch_index(self, coords, H, W):\n",
    "        \"\"\"Convert coordinates to patch indices\"\"\"\n",
    "        y, x = coords[:, 0], coords[:, 1]\n",
    "        row = (y * H).long().clamp(0, H-1)\n",
    "        col = (x * W).long().clamp(0, W-1)\n",
    "        return row * W + col\n",
    "    \n",
    "    def compute_spatial_bias(self, target_index, H, W, num_tokens):\n",
    "        \"\"\"Compute spatial bias for attention\"\"\"\n",
    "        N = H * W\n",
    "        t = target_index.float() / N\n",
    "        token_positions = torch.linspace(0.5/num_tokens, 1 - 0.5/num_tokens, \n",
    "                                        num_tokens, device=target_index.device)\n",
    "        distances = torch.abs(t.unsqueeze(0) - token_positions.unsqueeze(1))\n",
    "        return -self.alpha * distances**2\n",
    "    \n",
    "    def cross_attention(self, queries, context, bias=None):\n",
    "        \"\"\"Cross-attention with optional spatial bias\"\"\"\n",
    "        B, N, D = queries.shape\n",
    "        \n",
    "        q = self.to_q(queries)  # (B, N, D)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)  # (B, L, D)\n",
    "        \n",
    "        sim = torch.einsum('bnd,bld->bnl', q, k) * self.scale\n",
    "        \n",
    "        if bias is not None:\n",
    "            sim = sim + bias.unsqueeze(0)\n",
    "        \n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('bnl,bld->bnd', attn, v)\n",
    "        return self.attn_out(out)\n",
    "    \n",
    "    def forward(self, coords, tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords: (B, H, W, 2) query coordinates\n",
    "            tokens: (B, L, D) LP token features\n",
    "        Returns:\n",
    "            rgb: (B, H, W, 3) predicted RGB values\n",
    "        \"\"\"\n",
    "        B, H, W, _ = coords.shape\n",
    "        coords_flat = coords.reshape(B, -1, 2)\n",
    "        \n",
    "        # Fourier encoding\n",
    "        fourier_features = fourier_encode(coords_flat[0], self.n_features)\n",
    "        fourier_features = repeat(fourier_features, 'n d -> b n d', b=B)\n",
    "        \n",
    "        # Query projection\n",
    "        queries = F.relu(self.query_proj(fourier_features))\n",
    "        \n",
    "        # Spatial bias\n",
    "        indices = self.get_patch_index(coords_flat[0], self.patch_num, self.patch_num)\n",
    "        bias = self.compute_spatial_bias(indices, self.patch_num, self.patch_num, tokens.shape[1])\n",
    "        \n",
    "        # Extract modulation via cross-attention\n",
    "        modulation = self.cross_attention(queries, tokens, bias)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_input = torch.cat([fourier_features, modulation], dim=-1)\n",
    "        features = self.decoder_blocks(decoder_input)\n",
    "        rgb = self.output_proj(features)\n",
    "        \n",
    "        return rgb.reshape(B, H, W, 3)\n",
    "\n",
    "print(\"✓ Original LAINR decoder defined (ResidualBlock-based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Complete MAMBA-GINR Model\n",
    "\n",
    "class MambaGINR(nn.Module):\n",
    "    \"\"\"Original MAMBA-GINR architecture\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_size=32,\n",
    "                 patch_size=2,\n",
    "                 in_channels=3,\n",
    "                 dim=256,\n",
    "                 num_lp=256,\n",
    "                 mamba_depth=6,\n",
    "                 ff_dim=1024,\n",
    "                 hidden_dim=512,\n",
    "                 n_features=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_num = img_size // patch_size\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Linear(patch_size * patch_size * in_channels, dim)\n",
    "        \n",
    "        # Patch positional encoding\n",
    "        self.register_buffer('pos_freq', torch.randn(dim // 2, 2) * 10.0)\n",
    "        self.pos_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Learnable position tokens\n",
    "        self.lp_module = LearnablePositionTokens(\n",
    "            num_tokens=num_lp,\n",
    "            dim=dim,\n",
    "            input_len=self.num_patches\n",
    "        )\n",
    "        \n",
    "        # Mamba encoder\n",
    "        self.encoder = MambaEncoder(\n",
    "            depth=mamba_depth,\n",
    "            dim=dim,\n",
    "            ff_dim=ff_dim\n",
    "        )\n",
    "        \n",
    "        # LAINR decoder\n",
    "        self.decoder = LAINRDecoder(\n",
    "            n_features=n_features,\n",
    "            input_dim=2,\n",
    "            output_dim=3,\n",
    "            hidden_dim=hidden_dim,\n",
    "            context_dim=dim,\n",
    "            n_patches=self.num_patches\n",
    "        )\n",
    "    \n",
    "    def patchify(self, images):\n",
    "        \"\"\"Convert images to patches\"\"\"\n",
    "        B, C, H, W = images.shape\n",
    "        p = self.patch_size\n",
    "        patches = rearrange(images, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        return patches\n",
    "    \n",
    "    def get_patch_positions(self, B, device):\n",
    "        \"\"\"Get normalized patch center positions\"\"\"\n",
    "        h = w = self.patch_num\n",
    "        y = torch.linspace(0.5/h, 1 - 0.5/h, h, device=device)\n",
    "        x = torch.linspace(0.5/w, 1 - 0.5/w, w, device=device)\n",
    "        yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "        positions = torch.stack([yy, xx], dim=-1).reshape(-1, 2)\n",
    "        return positions.unsqueeze(0).expand(B, -1, -1)\n",
    "    \n",
    "    def fourier_pos_encoding(self, positions):\n",
    "        \"\"\"Fourier positional encoding for patches\"\"\"\n",
    "        proj = 2 * np.pi * positions @ self.pos_freq.T\n",
    "        encoding = torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)\n",
    "        return self.pos_proj(encoding)\n",
    "    \n",
    "    def encode(self, images):\n",
    "        \"\"\"Encode images to LP features\"\"\"\n",
    "        B = images.shape[0]\n",
    "        \n",
    "        # Patchify\n",
    "        patches = self.patchify(images)  # (B, num_patches, patch_dim)\n",
    "        tokens = self.patch_embed(patches)  # (B, num_patches, dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        positions = self.get_patch_positions(B, images.device)\n",
    "        pos_encoding = self.fourier_pos_encoding(positions)\n",
    "        tokens = tokens + pos_encoding\n",
    "        \n",
    "        # Add LP tokens\n",
    "        tokens_with_lp = self.lp_module.add_lp(tokens)\n",
    "        \n",
    "        # Encode with Mamba\n",
    "        encoded = self.encoder(tokens_with_lp)\n",
    "        \n",
    "        # Extract LP features\n",
    "        lp_features = self.lp_module.extract_lp(encoded)\n",
    "        \n",
    "        return lp_features\n",
    "    \n",
    "    def decode(self, lp_features, coords):\n",
    "        \"\"\"Decode LP features to RGB at coordinates\"\"\"\n",
    "        return self.decoder(coords, lp_features)\n",
    "    \n",
    "    def forward(self, images, coords):\n",
    "        \"\"\"Full forward pass\"\"\"\n",
    "        lp_features = self.encode(images)\n",
    "        return self.decode(lp_features, coords)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"✓ Complete MAMBA-GINR model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Functions\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr=5e-4, warmup_epochs=5, max_epoch=40):\n",
    "    \"\"\"Learning rate schedule: warmup + cosine annealing\"\"\"\n",
    "    min_lr = 1e-8\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        t = (epoch - warmup_epochs) / (max_epoch - warmup_epochs)\n",
    "        lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * t))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, epoch, resolution=32):\n",
    "    \"\"\"Training for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_psnr = 0, 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "        \n",
    "        # Query coordinates (exact pixel centers)\n",
    "        coords = create_coordinate_grid(resolution, resolution, device)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "        \n",
    "        # Forward\n",
    "        pred = model(images, coords_batch)\n",
    "        gt = rearrange(images, 'b c h w -> b h w c')\n",
    "        \n",
    "        # Loss\n",
    "        mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "        loss = mses.mean()\n",
    "        psnr = (-10 * torch.log10(mses)).mean()\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_psnr += psnr.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'psnr': f\"{psnr.item():.2f}\"})\n",
    "    \n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "def validate(model, loader, device, resolution=32):\n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_psnr = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            B = images.shape[0]\n",
    "            \n",
    "            coords = create_coordinate_grid(resolution, resolution, device)\n",
    "            coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "            \n",
    "            pred = model(images, coords_batch)\n",
    "            gt = rearrange(images, 'b c h w -> b h w c')\n",
    "            \n",
    "            mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "            total_loss += mses.mean().item()\n",
    "            total_psnr += (-10 * torch.log10(mses)).mean().item()\n",
    "    \n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "def super_resolve(model, images, target_resolution=128, device='cpu'):\n",
    "    \"\"\"Generate at arbitrary resolution\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B = images.shape[0]\n",
    "        lp_features = model.encode(images)\n",
    "        coords = create_coordinate_grid(target_resolution, target_resolution, device)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "        pred = model.decode(lp_features, coords_batch)\n",
    "        return rearrange(pred, 'b h w c -> b c h w')\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data Loading\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, \n",
    "                         num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, \n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model Initialization\n",
    "\n",
    "model = MambaGINR(\n",
    "    img_size=32,\n",
    "    patch_size=2,\n",
    "    in_channels=3,\n",
    "    dim=256,\n",
    "    num_lp=256,\n",
    "    mamba_depth=6,\n",
    "    ff_dim=1024,\n",
    "    hidden_dim=512,\n",
    "    n_features=32\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MAMBA-GINR ARCHITECTURE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"  - Encoder (BiMamba):  {encoder_params:,}\")\n",
    "print(f\"  - Decoder (LAINR):    {decoder_params:,}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Image size: 32×32\")\n",
    "print(f\"  - Patch size: 2×2\")\n",
    "print(f\"  - Num patches: {model.num_patches}\")\n",
    "print(f\"  - LP tokens: {model.lp_module.num_tokens}\")\n",
    "print(f\"  - Hidden dim: 256\")\n",
    "print(f\"  - Mamba depth: 6\")\n",
    "print(f\"\\nTarget: 60 PSNR on CIFAR-10\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training Loop\n",
    "\n",
    "num_epochs = 40\n",
    "best_val_psnr = 0\n",
    "train_losses, train_psnrs = [], []\n",
    "val_losses, val_psnrs = [], []\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, base_lr=5e-4, max_epoch=num_epochs)\n",
    "    train_loss, train_psnr = train_epoch(model, train_loader, optimizer, device, epoch+1)\n",
    "    val_loss, val_psnr = validate(model, test_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_psnrs.append(train_psnr)\n",
    "    val_losses.append(val_loss)\n",
    "    val_psnrs.append(val_psnr)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} | LR: {lr:.6f}\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
    "    \n",
    "    if val_psnr > best_val_psnr:\n",
    "        best_val_psnr = val_psnr\n",
    "        torch.save(model.state_dict(), 'mamba_ginr_best.pth')\n",
    "        print(f\"  → Best model saved (PSNR: {best_val_psnr:.2f} dB)\")\n",
    "    \n",
    "    # Plot every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        axes[0].plot(train_losses, label='Train')\n",
    "        axes[0].plot(val_losses, label='Val')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('MSE Loss')\n",
    "        axes[0].set_yscale('log')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        axes[0].set_title('Loss')\n",
    "        \n",
    "        axes[1].plot(train_psnrs, label='Train')\n",
    "        axes[1].plot(val_psnrs, label='Val')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('PSNR (dB)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "        axes[1].set_title('PSNR')\n",
    "        \n",
    "        # Sample reconstruction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample, _ = next(iter(test_loader))\n",
    "            sample = sample[:1].to(device)\n",
    "            coords = create_coordinate_grid(32, 32, device)\n",
    "            coords_batch = coords.unsqueeze(0)\n",
    "            pred = model(sample, coords_batch)\n",
    "            comparison = torch.cat([\n",
    "                sample[0].cpu().permute(1, 2, 0),\n",
    "                pred[0].cpu().clamp(0, 1)\n",
    "            ], dim=1)\n",
    "            axes[2].imshow(comparison)\n",
    "            axes[2].set_title('Original | Reconstruction')\n",
    "            axes[2].axis('off')\n",
    "        model.train()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best validation PSNR: {best_val_psnr:.2f} dB\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Super-Resolution Test\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('mamba_ginr_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get test samples\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images[:8].to(device)\n",
    "\n",
    "print(\"Generating super-resolution outputs...\\n\")\n",
    "\n",
    "# Generate at multiple resolutions\n",
    "sr_64 = super_resolve(model, test_images, 64, device)\n",
    "sr_128 = super_resolve(model, test_images, 128, device)\n",
    "sr_256 = super_resolve(model, test_images, 256, device)\n",
    "\n",
    "print(f\"Original: {test_images.shape}\")\n",
    "print(f\"SR 64×64: {sr_64.shape}\")\n",
    "print(f\"SR 128×128: {sr_128.shape}\")\n",
    "print(f\"SR 256×256: {sr_256.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(test_images[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].imshow(sr_64[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2, i].imshow(sr_128[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[3, i].imshow(sr_256[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    for j in range(4):\n",
    "        axes[j, i].axis('off')\n",
    "\n",
    "labels = ['32×32', '64×64 SR', '128×128 SR', '256×256 SR']\n",
    "for j, label in enumerate(labels):\n",
    "    axes[j, 0].set_ylabel(label, fontsize=12, rotation=0, labelpad=40)\n",
    "\n",
    "plt.suptitle('MAMBA-GINR: Super-Resolution Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mamba_ginr_super_resolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Results saved as 'mamba_ginr_super_resolution.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Final Analysis\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MAMBA-GINR CIFAR-10 Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  - Best validation PSNR: {best_val_psnr:.2f} dB\")\n",
    "print(f\"  - Final train PSNR: {train_psnrs[-1]:.2f} dB\")\n",
    "print(f\"  - Final val PSNR: {val_psnrs[-1]:.2f} dB\")\n",
    "print(f\"\\nArchitecture Summary:\")\n",
    "print(f\"  - BiMamba encoder: {encoder_params:,} params\")\n",
    "print(f\"  - LAINR decoder: {decoder_params:,} params\")\n",
    "print(f\"  - Total: {total_params:,} params\")\n",
    "print(f\"\\nKey Features:\")\n",
    "print(f\"  ✓ Learnable position tokens (implicit sequential bias)\")\n",
    "print(f\"  ✓ Bidirectional Mamba (O(L) complexity)\")\n",
    "print(f\"  ✓ Arbitrary-resolution generation\")\n",
    "print(f\"  ✓ ResidualBlock-based decoder\")\n",
    "print(f\"\\nTarget: 60 PSNR (requires perfect reconstruction)\")\n",
    "print(f\"Achieved: {best_val_psnr:.2f} dB\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
