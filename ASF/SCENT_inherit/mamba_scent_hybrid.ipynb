{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA-GINR Encoder + SCENT Processor/Decoder\n",
    "\n",
    "## Hybrid Architecture:\n",
    "1. **BiMamba Encoder** (from MAMBA-GINR): O(L) complexity, LP tokens, sequential bias\n",
    "2. **Self-Attention Processor** (from SCENT): 4 layers on 256 latents, global mixing\n",
    "3. **SCENT Decoder**: Gaussian Fourier, cross-attention, FeedForwards, skip connections\n",
    "\n",
    "## Expected Benefits:\n",
    "- Encoder efficiency (O(L)) + Decoder capacity (SCENT-style)\n",
    "- Best super-resolution: ~32-34 dB PSNR at 128×128\n",
    "- Memory efficient: ~6GB total (fits 40GB GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log, pi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def gaussian_fourier_encode(coords, B_matrix):\n",
    "    \"\"\"Gaussian Fourier Feature encoding\"\"\"\n",
    "    if coords.dim() == 3:\n",
    "        coords = coords.view(-1, coords.shape[-1])\n",
    "    proj = 2 * np.pi * coords @ B_matrix.T\n",
    "    return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "\n",
    "def create_coordinate_grid(H, W, device):\n",
    "    \"\"\"Create normalized coordinate grid in [0,1]\"\"\"\n",
    "    y = torch.linspace(0, 1, H, device=device)\n",
    "    x = torch.linspace(0, 1, W, device=device)\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    return torch.stack([yy, xx], dim=-1)\n",
    "\n",
    "def get_sinusoidal_embeddings(n, d):\n",
    "    \"\"\"Sinusoidal positional embeddings\"\"\"\n",
    "    assert d % 2 == 0\n",
    "    position = torch.arange(n, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d, 2).float() * -(log(10000.0) / d))\n",
    "    pe = torch.zeros(n, d)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: SCENT Building Blocks\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            kwargs.update(context=self.norm_context(context))\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, bias=None):\n",
    "        h = self.heads\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        \n",
    "        if exists(bias):\n",
    "            if bias.dim() == 3 and bias.shape[0] == x.shape[0]:\n",
    "                bias = repeat(bias, 'b l n -> (b h) l n', h=h)\n",
    "                bias = bias.transpose(-2, -1)\n",
    "            sim = sim + bias\n",
    "        \n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "        \n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "print(\"✓ SCENT blocks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: MAMBA-GINR Encoder Components\n",
    "\n",
    "class BiMamba(nn.Module):\n",
    "    \"\"\"Bidirectional Mamba from MAMBA-GINR\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.forward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.backward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.proj = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_forward = self.forward_mamba(x)\n",
    "        x_backward = self.backward_mamba(torch.flip(x, dims=[1]))\n",
    "        x_backward = torch.flip(x_backward, dims=[1])\n",
    "        x = torch.cat([x_forward, x_backward], dim=-1)\n",
    "        return self.proj(x)\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, patch_size=2, in_channels=3, dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_channels, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        p = self.patch_size\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        return self.proj(x)\n",
    "\n",
    "class LearnablePositionTokens(nn.Module):\n",
    "    \"\"\"LP tokens with sinusoidal initialization\"\"\"\n",
    "    def __init__(self, num_tokens=256, dim=256):\n",
    "        super().__init__()\n",
    "        init_tokens = get_sinusoidal_embeddings(num_tokens, dim)\n",
    "        self.tokens = nn.Parameter(init_tokens, requires_grad=True)\n",
    "\n",
    "    def forward(self, B):\n",
    "        return repeat(self.tokens, 'n d -> b n d', b=B)\n",
    "\n",
    "class MambaEncoder(nn.Module):\n",
    "    \"\"\"MAMBA-GINR style encoder\"\"\"\n",
    "    def __init__(self, patch_size=2, in_channels=3, dim=256, num_lp_tokens=256):\n",
    "        super().__init__()\n",
    "        self.patch_encoder = PatchEncoder(patch_size, in_channels, dim)\n",
    "        self.lp_tokens = LearnablePositionTokens(num_lp_tokens, dim)\n",
    "        self.mamba = BiMamba(d_model=dim)\n",
    "        self.num_lp_tokens = num_lp_tokens\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        patches = self.patch_encoder(x)\n",
    "        lp_tokens = self.lp_tokens(B)\n",
    "        combined = torch.cat([patches, lp_tokens], dim=1)\n",
    "        features = self.mamba(combined)\n",
    "        # Extract only LP tokens\n",
    "        return features[:, -self.num_lp_tokens:, :]\n",
    "\n",
    "print(\"✓ MAMBA-GINR encoder defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SCENT Processor\n",
    "\n",
    "class SCENTProcessor(nn.Module):\n",
    "    \"\"\"Self-attention processor from SCENT (4 layers)\"\"\"\n",
    "    def __init__(self, dim=256, num_layers=4, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head)),\n",
    "                PreNorm(dim, FeedForward(dim, mult=4))\n",
    "            ])\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "print(\"✓ SCENT processor defined (4 self-attention layers on latents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: SCENT Decoder (Memory Efficient)\n",
    "\n",
    "class SCENTDecoder(nn.Module):\n",
    "    \"\"\"SCENT-style decoder with Gaussian Fourier and skip connections\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=64, input_dim=2, output_dim=3,\n",
    "                 sigma_q=16, sigma_ls=[128, 32], n_patches=256,\n",
    "                 hidden_dim=512, context_dim=256,\n",
    "                 learnable_frequencies=True, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_num = len(sigma_ls)\n",
    "        self.n_features = feature_dim // 2\n",
    "        self.patch_num = int(math.sqrt(n_patches))\n",
    "        self.alpha = 10.0\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Gaussian Fourier frequencies\n",
    "        B_q_init = torch.randn(self.n_features, input_dim) / sigma_q\n",
    "        B_ls_init = [torch.randn(self.n_features, input_dim) / sigma_ls[i]\n",
    "                     for i in range(self.layer_num)]\n",
    "        \n",
    "        if learnable_frequencies:\n",
    "            self.B_q = nn.Parameter(B_q_init)\n",
    "            self.B_ls = nn.ParameterList([nn.Parameter(B_ls_init[i]) for i in range(self.layer_num)])\n",
    "        else:\n",
    "            self.register_buffer('B_q', B_q_init)\n",
    "            for i in range(self.layer_num):\n",
    "                self.register_buffer(f'B_l_{i}', B_ls_init[i])\n",
    "            self.B_ls = [getattr(self, f'B_l_{i}') for i in range(self.layer_num)]\n",
    "        \n",
    "        # Query encoding\n",
    "        self.query_lin = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        # Cross-attention for modulation\n",
    "        self.modulation_ca = PreNorm(\n",
    "            hidden_dim,\n",
    "            Attention(hidden_dim, context_dim, heads=2, dim_head=64),\n",
    "            context_dim=context_dim\n",
    "        )\n",
    "        \n",
    "        # Bandwidth encoders (FeedForward only for memory efficiency)\n",
    "        self.bandwidth_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[nn.Sequential(\n",
    "                    PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4)),\n",
    "                    PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))\n",
    "                ) for _ in range(num_layers - 1)]\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "        \n",
    "        # Modulation projections\n",
    "        self.modulation_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[nn.Sequential(\n",
    "                    PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4)),\n",
    "                    PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))\n",
    "                ) for _ in range(num_layers - 1)]\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "        \n",
    "        # Hidden value layers\n",
    "        self.hv_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4)),\n",
    "                PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))\n",
    "            )\n",
    "            for _ in range(self.layer_num - 1)\n",
    "        ])\n",
    "        \n",
    "        # Skip connection projections\n",
    "        self.fourier_skip_projs = nn.ModuleList([\n",
    "            nn.Linear(feature_dim, hidden_dim) for _ in range(self.layer_num)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.out_lins = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, output_dim) for _ in range(self.layer_num)\n",
    "        ])\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def get_patch_index(self, grid, H, W):\n",
    "        y, x = grid[:, 0], grid[:, 1]\n",
    "        row = (y * H).to(torch.int32).clamp(0, H-1)\n",
    "        col = (x * W).to(torch.int32).clamp(0, W-1)\n",
    "        return row * W + col\n",
    "    \n",
    "    def approximate_relative_distances(self, target_index, H, W, m):\n",
    "        N = H * W\n",
    "        t = target_index.float() / N\n",
    "        token_positions = torch.tensor([(i + 0.5) / m for i in range(m)], device=target_index.device)\n",
    "        t_expanded = t.unsqueeze(0)\n",
    "        tokens_expanded = token_positions.unsqueeze(1)\n",
    "        return -self.alpha * torch.abs(t_expanded - tokens_expanded)**2\n",
    "    \n",
    "    def apply_block_sequence(self, x, block_seq):\n",
    "        if isinstance(block_seq[0], nn.Linear):\n",
    "            x = block_seq[1](block_seq[0](x))\n",
    "            start_idx = 2\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        for i in range(start_idx, len(block_seq)):\n",
    "            block = block_seq[i]\n",
    "            if isinstance(block, nn.Sequential):\n",
    "                ff1, ff2 = block[0], block[1]\n",
    "                x = ff1(x) + x\n",
    "                x = ff2(x) + x\n",
    "        return x\n",
    "    \n",
    "    def forward(self, coords_decoding, tokens, coords_modulation=None):\n",
    "        B, query_shape = coords_decoding.shape[0], coords_decoding.shape[1:-1]\n",
    "        coords_dec = coords_decoding.view(B, -1, coords_decoding.shape[-1])\n",
    "        coords_mod = coords_modulation.view(B, -1, coords_modulation.shape[-1]) if coords_modulation is not None else coords_dec\n",
    "        \n",
    "        # Modulation extraction\n",
    "        grid_mod = coords_mod[0]\n",
    "        num_queries = grid_mod.shape[0]\n",
    "        H_mod = W_mod = int(math.sqrt(num_queries))\n",
    "        indexes = self.get_patch_index(grid_mod, H_mod, W_mod)\n",
    "        rel_distances = self.approximate_relative_distances(indexes, H_mod, W_mod, tokens.shape[1])\n",
    "        bias = repeat(rel_distances, 'l n -> b l n', b=B)\n",
    "        \n",
    "        x_q = repeat(gaussian_fourier_encode(coords_mod[0], self.B_q), 'l d -> b l d', b=B)\n",
    "        x_q = self.act(self.query_lin(x_q))\n",
    "        modulation_vector = self.modulation_ca(x_q, context=tokens, bias=bias)\n",
    "        \n",
    "        # Multi-scale decoding\n",
    "        modulations_l, fourier_encodings = [], []\n",
    "        for k in range(self.layer_num):\n",
    "            x_l_fourier = gaussian_fourier_encode(coords_dec[0], self.B_ls[k])\n",
    "            x_l_fourier_batch = repeat(x_l_fourier, 'l d -> b l d', b=B)\n",
    "            fourier_encodings.append(x_l_fourier_batch)\n",
    "            \n",
    "            h_l = self.apply_block_sequence(x_l_fourier_batch, self.bandwidth_lins[k])\n",
    "            m_proj = self.apply_block_sequence(modulation_vector, self.modulation_lins[k])\n",
    "            modulations_l.append(self.act(h_l + m_proj))\n",
    "        \n",
    "        # Residual connections\n",
    "        h_v = [modulations_l[0]]\n",
    "        for i in range(self.layer_num - 1):\n",
    "            x_combined = modulations_l[i+1] + h_v[i]\n",
    "            ff_block = self.hv_lins[i]\n",
    "            x_combined = ff_block[0](x_combined) + x_combined\n",
    "            x_combined = ff_block[1](x_combined) + x_combined\n",
    "            h_v.append(x_combined)\n",
    "        \n",
    "        # Output with skip connections\n",
    "        outs = []\n",
    "        for i in range(self.layer_num):\n",
    "            fourier_skip = self.fourier_skip_projs[i](fourier_encodings[i])\n",
    "            outs.append(self.out_lins[i](h_v[i] + fourier_skip))\n",
    "        \n",
    "        return sum(outs).view(B, *query_shape, -1)\n",
    "\n",
    "print(\"✓ SCENT decoder defined (with skip connections)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Complete Hybrid Model\n",
    "\n",
    "class MambaSCENTHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid Architecture:\n",
    "    - MAMBA-GINR Encoder (BiMamba + LP tokens)\n",
    "    - SCENT Processor (4 self-attention layers)\n",
    "    - SCENT Decoder (Gaussian Fourier + skip connections)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 patch_size=2,\n",
    "                 in_channels=3,\n",
    "                 dim=256,\n",
    "                 num_lp_tokens=256,\n",
    "                 feature_dim=64,\n",
    "                 sigma_q=16,\n",
    "                 sigma_ls=[128, 32],\n",
    "                 hidden_dim=512,\n",
    "                 num_processor_layers=4,\n",
    "                 num_decoder_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MAMBA-GINR Encoder\n",
    "        self.encoder = MambaEncoder(\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            dim=dim,\n",
    "            num_lp_tokens=num_lp_tokens\n",
    "        )\n",
    "        \n",
    "        # SCENT Processor\n",
    "        self.processor = SCENTProcessor(\n",
    "            dim=dim,\n",
    "            num_layers=num_processor_layers,\n",
    "            heads=8,\n",
    "            dim_head=64\n",
    "        )\n",
    "        \n",
    "        # SCENT Decoder\n",
    "        self.num_patches = (32 // patch_size) ** 2\n",
    "        self.decoder = SCENTDecoder(\n",
    "            feature_dim=feature_dim,\n",
    "            input_dim=2,\n",
    "            output_dim=3,\n",
    "            sigma_q=sigma_q,\n",
    "            sigma_ls=sigma_ls,\n",
    "            n_patches=self.num_patches,\n",
    "            hidden_dim=hidden_dim,\n",
    "            context_dim=dim,\n",
    "            learnable_frequencies=True,\n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode image to latents\"\"\"\n",
    "        latents = self.encoder(x)\n",
    "        latents = self.processor(latents)\n",
    "        return latents\n",
    "    \n",
    "    def forward(self, x, coords_decoding, coords_modulation=None):\n",
    "        latents = self.encode(x)\n",
    "        return self.decoder(coords_decoding, latents, coords_modulation)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"✓ Hybrid model defined\")\n",
    "print(\"  Architecture: BiMamba Encoder → SCENT Processor → SCENT Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Functions\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr=5e-4, warmup_epochs=5, max_epoch=40):\n",
    "    min_lr = 1e-8\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        t = (epoch - warmup_epochs) / (max_epoch - warmup_epochs)\n",
    "        lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * t))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, epoch, resolution=32):\n",
    "    model.train()\n",
    "    total_loss, total_psnr = 0, 0\n",
    "    jitter_std = (1.0 / resolution) / 6\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "        base_coords = create_coordinate_grid(resolution, resolution, device)\n",
    "        jitter = torch.randn_like(base_coords) * jitter_std\n",
    "        coords = (base_coords + jitter).clamp(0, 1)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "\n",
    "        pred = model(images, coords_batch, coords_batch)\n",
    "        gt = rearrange(images, 'b c h w -> b h w c')\n",
    "        mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "        loss = mses.mean()\n",
    "        psnr = (-10 * torch.log10(mses)).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_psnr += psnr.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'psnr': f\"{psnr.item():.2f}\"})\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "def validate(model, loader, device, resolution=32):\n",
    "    model.eval()\n",
    "    total_loss, total_psnr = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            B = images.shape[0]\n",
    "            coords = create_coordinate_grid(resolution, resolution, device)\n",
    "            coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "            pred = model(images, coords_batch, None)\n",
    "            gt = rearrange(images, 'b c h w -> b h w c')\n",
    "            mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "            total_loss += mses.mean().item()\n",
    "            total_psnr += (-10 * torch.log10(mses)).mean().item()\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "def super_resolve(model, images, target_resolution=128, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B = images.shape[0]\n",
    "        latents = model.encode(images)\n",
    "        coords = create_coordinate_grid(target_resolution, target_resolution, device)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "        pred = model.decoder(coords_batch, latents, None)\n",
    "        return rearrange(pred, 'b h w c -> b c h w')\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data Loading\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model Initialization\n",
    "\n",
    "model = MambaSCENTHybrid(\n",
    "    patch_size=2,\n",
    "    in_channels=3,\n",
    "    dim=256,\n",
    "    num_lp_tokens=256,\n",
    "    feature_dim=64,\n",
    "    sigma_q=16,\n",
    "    sigma_ls=[128, 32],\n",
    "    hidden_dim=512,\n",
    "    num_processor_layers=4,\n",
    "    num_decoder_layers=3\n",
    ").to(device)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "processor_params = count_parameters(model.processor)\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"HYBRID MODEL ARCHITECTURE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"  - Encoder (BiMamba):    {encoder_params:,}\")\n",
    "print(f\"  - Processor (SCENT):    {processor_params:,}\")\n",
    "print(f\"  - Decoder (SCENT):      {decoder_params:,}\")\n",
    "print(f\"\\nExpected super-resolution PSNR at 128×128: ~32-34 dB\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training Loop\n",
    "\n",
    "num_epochs = 40\n",
    "best_val_psnr = 0\n",
    "\n",
    "print(f\"Training Mamba-SCENT Hybrid for {num_epochs} epochs\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, base_lr=5e-4, max_epoch=num_epochs)\n",
    "    train_loss, train_psnr = train_epoch(model, train_loader, optimizer, device, epoch+1)\n",
    "    val_loss, val_psnr = validate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} | LR: {lr:.6f}\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
    "    \n",
    "    if val_psnr > best_val_psnr:\n",
    "        best_val_psnr = val_psnr\n",
    "        torch.save(model.state_dict(), 'mamba_scent_hybrid_best.pth')\n",
    "        print(f\"  → Best model saved (PSNR: {best_val_psnr:.2f} dB)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete! Best validation PSNR: {best_val_psnr:.2f} dB\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Super-Resolution Test\n",
    "\n",
    "model.load_state_dict(torch.load('mamba_scent_hybrid_best.pth'))\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images[:8].to(device)\n",
    "\n",
    "print(\"Testing super-resolution...\\n\")\n",
    "\n",
    "sr_64 = super_resolve(model, test_images, 64, device)\n",
    "sr_128 = super_resolve(model, test_images, 128, device)\n",
    "sr_256 = super_resolve(model, test_images, 256, device)\n",
    "\n",
    "print(f\"Original: {test_images.shape}\")\n",
    "print(f\"SR 64×64: {sr_64.shape}\")\n",
    "print(f\"SR 128×128: {sr_128.shape}\")\n",
    "print(f\"SR 256×256: {sr_256.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(test_images[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].imshow(sr_64[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2, i].imshow(sr_128[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[3, i].imshow(sr_256[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    for j in range(4):\n",
    "        axes[j, i].axis('off')\n",
    "\n",
    "labels = ['32×32', '64×64 SR', '128×128 SR', '256×256 SR']\n",
    "for j, label in enumerate(labels):\n",
    "    axes[j, 0].set_ylabel(label, fontsize=12, rotation=0, labelpad=30)\n",
    "\n",
    "plt.suptitle('Mamba-SCENT Hybrid: Super-Resolution Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mamba_scent_hybrid_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Results saved as 'mamba_scent_hybrid_results.png'\")\n",
    "print(\"\\nExpected improvements:\")\n",
    "print(\"  ✓ Sharp high-frequency details\")\n",
    "print(\"  ✓ Better texture synthesis than MAMBA-GINR\")\n",
    "print(\"  ✓ More efficient than pure SCENT\")\n",
    "print(\"  ✓ ~32-34 dB PSNR at 128×128\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
