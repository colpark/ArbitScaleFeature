{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA-GINR with SCENT-Style Decoder\n",
    "\n",
    "## Key Changes from Previous Version:\n",
    "\n",
    "### ❌ Previous (Failed to Learn):\n",
    "- ResidualBlock: 512 → 512 → 512 (no expansion)\n",
    "- No attention (local processing only)\n",
    "- No skip connection from Fourier encoding to output\n",
    "- Result: Smooth upsampling, no high-frequency details\n",
    "\n",
    "### ✅ New (SCENT-Inspired):\n",
    "1. **Attention + FeedForward blocks** instead of ResidualBlock\n",
    "   - Self-attention: Global receptive field\n",
    "   - FeedForward: 512 → 2048 → 512 (4x expansion)\n",
    "   - GEGLU gating instead of ReLU\n",
    "   - LayerNorm for stability\n",
    "\n",
    "2. **Skip connection from Fourier encoding to output**\n",
    "   - Preserves high-frequency information\n",
    "   - Like SCENT: `x = x + queries`\n",
    "\n",
    "3. **Self-attention layers on latents**\n",
    "   - 2-4 layers of self-attention after BiMamba\n",
    "   - Global mixing of latent features\n",
    "\n",
    "4. **Sinusoidal initialization** for LP tokens\n",
    "   - Frequency-aware initialization\n",
    "   - Better alignment with Fourier features\n",
    "\n",
    "## Expected Improvement:\n",
    "- **+5-10 dB PSNR** at 128×128 super-resolution\n",
    "- Sharp high-frequency details instead of smooth blur\n",
    "- Better texture synthesis\n",
    "\n",
    "## Architecture Comparison:\n",
    "- Previous decoder: ~9M params (ResidualBlocks)\n",
    "- SCENT decoder: ~10M params (Attention + FF)\n",
    "- This decoder: ~10M params (Attention + FF, SCENT-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log, pi\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions and Gaussian Fourier Encoding\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def gaussian_fourier_encode(coords, B_matrix):\n",
    "    \"\"\"\n",
    "    Gaussian Fourier Feature encoding (Rahimi & Recht, 2007)\n",
    "    \n",
    "    Args:\n",
    "        coords: (H, W, 2) or (HW, 2) - normalized coordinates in [0,1]\n",
    "        B_matrix: (n_features, 2) - random frequency matrix\n",
    "    \n",
    "    Returns:\n",
    "        features: (HW, 2*n_features) - Fourier features\n",
    "    \"\"\"\n",
    "    if coords.dim() == 3:\n",
    "        coords = coords.view(-1, coords.shape[-1])\n",
    "    \n",
    "    proj = 2 * np.pi * coords @ B_matrix.T\n",
    "    features = torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "    return features\n",
    "\n",
    "\n",
    "def create_coordinate_grid(H, W, device):\n",
    "    \"\"\"Create normalized coordinate grid in [0,1]\"\"\"\n",
    "    y = torch.linspace(0, 1, H, device=device)\n",
    "    x = torch.linspace(0, 1, W, device=device)\n",
    "    yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "    coords = torch.stack([yy, xx], dim=-1)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def get_sinusoidal_embeddings(n, d):\n",
    "    \"\"\"\n",
    "    Generates sinusoidal positional embeddings (like SCENT).\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of positions (num_latents)\n",
    "        d (int): Embedding dimension (latent_dim)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: (n, d) with sinusoidal embeddings\n",
    "    \"\"\"\n",
    "    assert d % 2 == 0, \"d must be even for sinusoidal embeddings\"\n",
    "    \n",
    "    position = torch.arange(n, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d, 2).float() * -(log(10000.0) / d))\n",
    "    \n",
    "    pe = torch.zeros(n, d)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: SCENT-Style Building Blocks\n\nclass PreNorm(nn.Module):\n    \"\"\"Pre-normalization wrapper (from SCENT)\"\"\"\n    def __init__(self, dim, fn, context_dim=None):\n        super().__init__()\n        self.fn = fn\n        self.norm = nn.LayerNorm(dim)\n        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        if exists(self.norm_context):\n            context = kwargs['context']\n            normed_context = self.norm_context(context)\n            kwargs.update(context=normed_context)\n        return self.fn(x, **kwargs)\n\n\nclass GEGLU(nn.Module):\n    \"\"\"Gated Linear Unit with GELU (from SCENT)\"\"\"\n    def forward(self, x):\n        x, gates = x.chunk(2, dim=-1)\n        return x * F.gelu(gates)\n\n\nclass FeedForward(nn.Module):\n    \"\"\"FeedForward with 4x expansion and GEGLU (from SCENT)\"\"\"\n    def __init__(self, dim, mult=4):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult * 2),  # *2 for GEGLU\n            GEGLU(),\n            nn.Linear(dim * mult, dim)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\n\nclass Attention(nn.Module):\n    \"\"\"Multi-head attention (from SCENT)\"\"\"\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        \n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, query_dim)\n\n    def forward(self, x, context=None, mask=None, bias=None):\n        h = self.heads\n        q = self.to_q(x)\n        context = default(context, x)\n        k, v = self.to_kv(context).chunk(2, dim=-1)\n        \n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n        \n        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n        \n        if exists(bias):\n            # Handle spatial bias for cross-attention\n            if bias.dim() == 3:  # (b, l, n)\n                bias = repeat(bias, 'b l n -> (b h) l n', h=h)\n                bias = bias.transpose(-2, -1)  # FIX: transpose to match sim dimensions\n            sim = sim + bias\n        \n        if exists(mask):\n            mask = rearrange(mask, 'b ... -> b (...)')\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n            sim.masked_fill_(~mask, max_neg_value)\n        \n        attn = sim.softmax(dim=-1)\n        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n        return self.to_out(out)\n\n\nprint(\"✓ SCENT-style building blocks defined\")\nprint(\"  - PreNorm (LayerNorm wrapper)\")\nprint(\"  - GEGLU (gated activation)\")\nprint(\"  - FeedForward (4x expansion)\")\nprint(\"  - Attention (multi-head with FIXED bias transpose)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: BiMamba Encoder with Self-Attention\n",
    "\n",
    "class BiMamba(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.forward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.backward_mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.proj = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_forward = self.forward_mamba(x)\n",
    "        x_backward = self.backward_mamba(torch.flip(x, dims=[1]))\n",
    "        x_backward = torch.flip(x_backward, dims=[1])\n",
    "        x = torch.cat([x_forward, x_backward], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, patch_size=2, in_channels=3, dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Linear(patch_size * patch_size * in_channels, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        p = self.patch_size\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LearnablePositionTokens(nn.Module):\n",
    "    \"\"\"LP tokens with sinusoidal initialization (SCENT-style)\"\"\"\n",
    "    def __init__(self, num_tokens=256, dim=256, use_sinusoidal=True):\n",
    "        super().__init__()\n",
    "        if use_sinusoidal:\n",
    "            init_tokens = get_sinusoidal_embeddings(num_tokens, dim)\n",
    "        else:\n",
    "            init_tokens = torch.randn(num_tokens, dim) * 0.02\n",
    "        \n",
    "        self.tokens = nn.Parameter(init_tokens, requires_grad=True)\n",
    "\n",
    "    def forward(self, B):\n",
    "        return repeat(self.tokens, 'n d -> b n d', b=B)\n",
    "\n",
    "\n",
    "class LatentProcessor(nn.Module):\n",
    "    \"\"\"Self-attention layers on latents (inspired by SCENT)\"\"\"\n",
    "    def __init__(self, dim, num_layers=2, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head)),\n",
    "                PreNorm(dim, FeedForward(dim, mult=4))\n",
    "            ])\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ Encoder components defined\")\n",
    "print(\"  - BiMamba (bidirectional SSM)\")\n",
    "print(\"  - PatchEncoder (image to patches)\")\n",
    "print(\"  - LearnablePositionTokens (sinusoidal init)\")\n",
    "print(\"  - LatentProcessor (self-attention layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SCENT-Style Decoder with Skip Connections\n",
    "\n",
    "class LAINRDecoderSCENT(nn.Module):\n",
    "    \"\"\"\n",
    "    LAINR Decoder with SCENT-style architecture\n",
    "    \n",
    "    Key improvements over ResidualBlock version:\n",
    "    1. Attention + FeedForward blocks (not ResidualBlocks)\n",
    "    2. 4x expansion in FeedForward (512 → 2048 → 512)\n",
    "    3. Skip connection from Fourier encoding to output\n",
    "    4. GEGLU gating, LayerNorm, self-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=64, input_dim=2, output_dim=3,\n",
    "                 sigma_q=16, sigma_ls=[128, 32], n_patches=256,\n",
    "                 hidden_dim=512,\n",
    "                 context_dim=256,\n",
    "                 learnable_frequencies=True,\n",
    "                 num_layers=3,\n",
    "                 heads=8,\n",
    "                 dim_head=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_num = len(sigma_ls)\n",
    "        self.n_features = feature_dim // 2\n",
    "        self.patch_num = int(math.sqrt(n_patches))\n",
    "        self.alpha = 10.0\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize Gaussian Fourier frequency matrices\n",
    "        B_q_init = torch.randn(self.n_features, input_dim) / sigma_q\n",
    "        B_ls_init = [torch.randn(self.n_features, input_dim) / sigma_ls[i]\n",
    "                     for i in range(self.layer_num)]\n",
    "\n",
    "        if learnable_frequencies:\n",
    "            self.B_q = nn.Parameter(B_q_init)\n",
    "            self.B_ls = nn.ParameterList([\n",
    "                nn.Parameter(B_ls_init[i]) for i in range(self.layer_num)\n",
    "            ])\n",
    "        else:\n",
    "            self.register_buffer('B_q', B_q_init)\n",
    "            for i in range(self.layer_num):\n",
    "                self.register_buffer(f'B_l_{i}', B_ls_init[i])\n",
    "            self.B_ls = [getattr(self, f'B_l_{i}') for i in range(self.layer_num)]\n",
    "\n",
    "        # Query encoding\n",
    "        self.query_lin = nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "        # Cross-attention for modulation extraction\n",
    "        self.modulation_ca = PreNorm(\n",
    "            hidden_dim,\n",
    "            Attention(hidden_dim, context_dim, heads=2, dim_head=64),\n",
    "            context_dim=context_dim\n",
    "        )\n",
    "\n",
    "        # Bandwidth encoders - SCENT-STYLE (Attention + FF)\n",
    "        self.bandwidth_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(feature_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[\n",
    "                    nn.ModuleList([\n",
    "                        PreNorm(hidden_dim, Attention(hidden_dim, heads=heads, dim_head=dim_head)),\n",
    "                        PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))\n",
    "                    ])\n",
    "                    for _ in range(num_layers - 1)\n",
    "                ]\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        # Modulation projections - SCENT-STYLE\n",
    "        self.modulation_lins = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[\n",
    "                    nn.ModuleList([\n",
    "                        PreNorm(hidden_dim, Attention(hidden_dim, heads=heads, dim_head=dim_head)),\n",
    "                        PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))\n",
    "                    ])\n",
    "                    for _ in range(num_layers - 1)\n",
    "                ]\n",
    "            )\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        # Hidden value layers - SCENT-STYLE\n",
    "        self.hv_lins = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                PreNorm(hidden_dim, Attention(hidden_dim, heads=heads, dim_head=dim_head)),\n",
    "                PreNorm(hidden_dim, FeedForward(hidden_dim, mult=4))\n",
    "            ])\n",
    "            for _ in range(self.layer_num - 1)\n",
    "        ])\n",
    "\n",
    "        # Skip connection projections (KEY INNOVATION from SCENT!)\n",
    "        self.fourier_skip_projs = nn.ModuleList([\n",
    "            nn.Linear(feature_dim, hidden_dim) for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        # Output layers\n",
    "        self.out_lins = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "            for _ in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def get_patch_index(self, grid, H, W):\n",
    "        \"\"\"Convert coordinates to patch indices\"\"\"\n",
    "        y = grid[:, 0]\n",
    "        x = grid[:, 1]\n",
    "        row = (y * H).to(torch.int32).clamp(0, H-1)\n",
    "        col = (x * W).to(torch.int32).clamp(0, W-1)\n",
    "        return row * W + col\n",
    "\n",
    "    def approximate_relative_distances(self, target_index, H, W, m):\n",
    "        \"\"\"Compute spatial bias\"\"\"\n",
    "        alpha = self.alpha\n",
    "        N = H * W\n",
    "        t = target_index.float() / N\n",
    "\n",
    "        token_positions = torch.tensor(\n",
    "            [(i + 0.5) / m for i in range(m)],\n",
    "            device=target_index.device\n",
    "        )\n",
    "\n",
    "        t_expanded = t.unsqueeze(0)\n",
    "        tokens_expanded = token_positions.unsqueeze(1)\n",
    "        rel_distances = -alpha * torch.abs(t_expanded - tokens_expanded)**2\n",
    "\n",
    "        return rel_distances\n",
    "    \n",
    "    def apply_block_sequence(self, x, block_seq):\n",
    "        \"\"\"Apply sequence of Attention + FeedForward blocks\"\"\"\n",
    "        # First layer is Linear + ReLU\n",
    "        if isinstance(block_seq[0], nn.Linear):\n",
    "            x = block_seq[1](block_seq[0](x))  # Linear + ReLU\n",
    "            start_idx = 2\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        \n",
    "        # Rest are Attention + FF blocks\n",
    "        for i in range(start_idx, len(block_seq)):\n",
    "            block_list = block_seq[i]\n",
    "            if isinstance(block_list, nn.ModuleList):\n",
    "                attn, ff = block_list[0], block_list[1]\n",
    "                x = attn(x) + x\n",
    "                x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "    def forward(self, coords_decoding, tokens, coords_modulation=None):\n",
    "        \"\"\"\n",
    "        Forward pass with SCENT-style architecture\n",
    "\n",
    "        Args:\n",
    "            coords_decoding: (B, H, W, 2) - where to predict RGB\n",
    "            tokens: (B, L, D) - LP token features\n",
    "            coords_modulation: (B, H, W, 2) - where to extract modulation (None = test mode)\n",
    "        \"\"\"\n",
    "        B, query_shape = coords_decoding.shape[0], coords_decoding.shape[1:-1]\n",
    "        coords_dec = coords_decoding.view(B, -1, coords_decoding.shape[-1])\n",
    "\n",
    "        if coords_modulation is not None:\n",
    "            coords_mod = coords_modulation.view(B, -1, coords_modulation.shape[-1])\n",
    "        else:\n",
    "            coords_mod = coords_dec\n",
    "\n",
    "        # === MODULATION EXTRACTION ===\n",
    "\n",
    "        grid_mod = coords_mod[0]\n",
    "        indexes = self.get_patch_index(grid_mod, self.patch_num, self.patch_num)\n",
    "        rel_distances = self.approximate_relative_distances(\n",
    "            indexes, self.patch_num, self.patch_num, tokens.shape[1]\n",
    "        )\n",
    "        bias = repeat(rel_distances, 'l n -> b l n', b=B)\n",
    "\n",
    "        # Query encoding\n",
    "        x_q = repeat(\n",
    "            gaussian_fourier_encode(coords_mod[0], self.B_q), 'l d -> b l d', b=B\n",
    "        )\n",
    "        x_q = self.act(self.query_lin(x_q))\n",
    "\n",
    "        # Extract modulation\n",
    "        modulation_vector = self.modulation_ca(x_q, context=tokens, bias=bias)\n",
    "\n",
    "        # === MULTI-SCALE DECODING (SCENT-STYLE) ===\n",
    "\n",
    "        modulations_l = []\n",
    "        fourier_encodings = []  # Save for skip connections!\n",
    "        \n",
    "        for k in range(self.layer_num):\n",
    "            # Bandwidth encoding (Fourier features)\n",
    "            x_l_fourier = gaussian_fourier_encode(coords_dec[0], self.B_ls[k])\n",
    "            x_l_fourier_batch = repeat(x_l_fourier, 'l d -> b l d', b=B)\n",
    "            fourier_encodings.append(x_l_fourier_batch)\n",
    "            \n",
    "            # Process through Attention + FF blocks\n",
    "            h_l = self.apply_block_sequence(x_l_fourier_batch, self.bandwidth_lins[k])\n",
    "\n",
    "            # Modulation projection (Attention + FF blocks)\n",
    "            m_proj = self.apply_block_sequence(modulation_vector, self.modulation_lins[k])\n",
    "\n",
    "            # Combine\n",
    "            m_l = self.act(h_l + m_proj)\n",
    "            modulations_l.append(m_l)\n",
    "\n",
    "        # Residual connections between scales\n",
    "        h_v = [modulations_l[0]]\n",
    "        for i in range(self.layer_num - 1):\n",
    "            x_combined = modulations_l[i+1] + h_v[i]\n",
    "            \n",
    "            # Apply Attention + FF\n",
    "            attn, ff = self.hv_lins[i][0], self.hv_lins[i][1]\n",
    "            x_combined = attn(x_combined) + x_combined\n",
    "            x_combined = ff(x_combined) + x_combined\n",
    "            \n",
    "            h_v.append(x_combined)\n",
    "\n",
    "        # Output with SKIP CONNECTIONS (like SCENT!)\n",
    "        outs = []\n",
    "        for i in range(self.layer_num):\n",
    "            # Process through network\n",
    "            out_processed = self.out_lins[i](h_v[i])\n",
    "            \n",
    "            # Add skip connection from original Fourier encoding\n",
    "            fourier_skip = self.fourier_skip_projs[i](fourier_encodings[i])\n",
    "            out_with_skip = self.out_lins[i](h_v[i] + fourier_skip)\n",
    "            \n",
    "            outs.append(out_with_skip)\n",
    "        \n",
    "        out = sum(outs)\n",
    "        out = out.view(B, *query_shape, -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"✓ SCENT-style decoder defined\")\n",
    "print(\"  Key improvements:\")\n",
    "print(\"  1. Attention + FeedForward blocks (not ResidualBlocks)\")\n",
    "print(\"  2. 4x expansion in FeedForward (512 → 2048 → 512)\")\n",
    "print(\"  3. Skip connection from Fourier encoding to output\")\n",
    "print(\"  4. GEGLU gating, LayerNorm, self-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Complete Model\n",
    "\n",
    "class MambaGINR_SCENT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 patch_size=2,\n",
    "                 in_channels=3,\n",
    "                 dim=256,\n",
    "                 num_lp_tokens=256,\n",
    "                 feature_dim=64,\n",
    "                 sigma_q=16,\n",
    "                 sigma_ls=[128, 32],\n",
    "                 hidden_dim=512,\n",
    "                 num_layers=3,\n",
    "                 num_latent_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.patch_encoder = PatchEncoder(patch_size=patch_size, in_channels=in_channels, dim=dim)\n",
    "        self.lp_tokens = LearnablePositionTokens(num_tokens=num_lp_tokens, dim=dim, use_sinusoidal=True)\n",
    "        self.mamba = BiMamba(d_model=dim)\n",
    "        \n",
    "        # NEW: Self-attention layers on latents (like SCENT)\n",
    "        self.latent_processor = LatentProcessor(dim=dim, num_layers=num_latent_layers, heads=8, dim_head=64)\n",
    "        \n",
    "        # Decoder (SCENT-STYLE)\n",
    "        self.num_patches = (32 // patch_size) ** 2\n",
    "        self.hyponet = LAINRDecoderSCENT(\n",
    "            feature_dim=feature_dim,\n",
    "            input_dim=2,\n",
    "            output_dim=3,\n",
    "            sigma_q=sigma_q,\n",
    "            sigma_ls=sigma_ls,\n",
    "            n_patches=self.num_patches,\n",
    "            hidden_dim=hidden_dim,\n",
    "            context_dim=dim,\n",
    "            learnable_frequencies=True,\n",
    "            num_layers=num_layers,\n",
    "            heads=8,\n",
    "            dim_head=64\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        B = x.shape[0]\n",
    "        patch_features = self.patch_encoder(x)\n",
    "        lp_tokens = self.lp_tokens(B)\n",
    "        combined = torch.cat([patch_features, lp_tokens], dim=1)\n",
    "        \n",
    "        # BiMamba processing\n",
    "        features = self.mamba(combined)\n",
    "        \n",
    "        # NEW: Self-attention layers on latents (like SCENT)\n",
    "        features = self.latent_processor(features)\n",
    "        \n",
    "        lp_features = features[:, -lp_tokens.shape[1]:, :]\n",
    "        return lp_features\n",
    "\n",
    "    def forward(self, x, coords_decoding, coords_modulation=None):\n",
    "        lp_features = self.encode(x)\n",
    "        rgb = self.hyponet(coords_decoding, lp_features, coords_modulation)\n",
    "        return rgb\n",
    "\n",
    "\n",
    "print(\"✓ Complete SCENT-style model defined\")\n",
    "print(\"  Architecture: BiMamba → Self-Attention → SCENT-Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Functions\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, base_lr=5e-4, warmup_epochs=5, max_epoch=40):\n",
    "    \"\"\"Learning rate schedule with warmup + cosine annealing\"\"\"\n",
    "    min_lr = 1e-8\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        t = (epoch - warmup_epochs) / (max_epoch - warmup_epochs)\n",
    "        lr = min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * t))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, epoch, resolution=32):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_psnr = 0\n",
    "\n",
    "    # Auto-compute jittering std\n",
    "    pixel_size = 1.0 / resolution\n",
    "    max_allowed_jitter = pixel_size / 2\n",
    "    jitter_std = max_allowed_jitter / 3  # 3-sigma rule\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Create base coordinate grid INSIDE loop\n",
    "        base_coords = create_coordinate_grid(resolution, resolution, device)\n",
    "\n",
    "        # Apply jittering for modulation extraction\n",
    "        jitter_small = torch.randn_like(base_coords) * jitter_std\n",
    "        coords_modulation = (base_coords + jitter_small).clamp(0, 1)\n",
    "        coords_decoding = coords_modulation  # No prediction offset\n",
    "\n",
    "        # Repeat for batch\n",
    "        coords_mod_batch = repeat(coords_modulation, 'h w d -> b h w d', b=B)\n",
    "        coords_dec_batch = repeat(coords_decoding, 'h w d -> b h w d', b=B)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(images, coords_dec_batch, coords_mod_batch)\n",
    "\n",
    "        # Ground truth\n",
    "        gt = rearrange(images, 'b c h w -> b h w c')\n",
    "\n",
    "        # Loss\n",
    "        mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "        loss = mses.mean()\n",
    "        psnr = (-10 * torch.log10(mses)).mean()\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_psnr += psnr.item()\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'psnr': f\"{psnr.item():.2f}\"\n",
    "        })\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, device, resolution=32):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_psnr = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            B = images.shape[0]\n",
    "\n",
    "            # Exact coordinates (no jittering)\n",
    "            coords = create_coordinate_grid(resolution, resolution, device)\n",
    "            coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "\n",
    "            # Forward pass (coords_modulation=None → test mode)\n",
    "            pred = model(images, coords_batch, coords_modulation=None)\n",
    "\n",
    "            # Ground truth\n",
    "            gt = rearrange(images, 'b c h w -> b h w c')\n",
    "\n",
    "            # Metrics\n",
    "            mses = ((pred - gt)**2).view(B, -1).mean(dim=-1)\n",
    "            loss = mses.mean()\n",
    "            psnr = (-10 * torch.log10(mses)).mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_psnr += psnr.item()\n",
    "\n",
    "    return total_loss / len(loader), total_psnr / len(loader)\n",
    "\n",
    "\n",
    "def super_resolve(model, images, target_resolution=128, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Encode at training resolution\n",
    "        lp_features = model.encode(images)\n",
    "\n",
    "        # Decode at target resolution (no jittering)\n",
    "        coords = create_coordinate_grid(target_resolution, target_resolution, device)\n",
    "        coords_batch = repeat(coords, 'h w d -> b h w d', b=B)\n",
    "\n",
    "        # Predict (coords_modulation=None → test mode)\n",
    "        pred = model.hyponet(coords_batch, lp_features, coords_modulation=None)\n",
    "\n",
    "        # Rearrange to image format\n",
    "        pred_images = rearrange(pred, 'b h w c -> b c h w')\n",
    "\n",
    "    return pred_images\n",
    "\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data Loading\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Initialization\n",
    "\n",
    "print(\"Initializing SCENT-style model...\\n\")\n",
    "\n",
    "model = MambaGINR_SCENT(\n",
    "    patch_size=2,\n",
    "    in_channels=3,\n",
    "    dim=256,\n",
    "    num_lp_tokens=256,\n",
    "    feature_dim=64,\n",
    "    sigma_q=16,\n",
    "    sigma_ls=[128, 32],\n",
    "    hidden_dim=512,\n",
    "    num_layers=3,\n",
    "    num_latent_layers=2\n",
    ").to(device)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "decoder_params = count_parameters(model.hyponet)\n",
    "latent_params = count_parameters(model.latent_processor)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Decoder parameters: {decoder_params:,}\")\n",
    "print(f\"Latent processor parameters: {latent_params:,}\")\n",
    "print(f\"\\nKey architectural improvements:\")\n",
    "print(f\"  1. Attention + FeedForward blocks (4x expansion)\")\n",
    "print(f\"  2. Skip connections from Fourier encoding\")\n",
    "print(f\"  3. Self-attention on latents ({latent_params:,} params)\")\n",
    "print(f\"  4. Sinusoidal initialization for LP tokens\")\n",
    "print(f\"\\nExpected: +5-10 dB PSNR improvement at 128×128\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "print(\"\\n✓ Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Loop\n",
    "\n",
    "num_epochs = 40\n",
    "resolution = 32\n",
    "\n",
    "print(f\"\\nTraining SCENT-style decoder for {num_epochs} epochs\")\n",
    "print(f\"Resolution: {resolution}×{resolution}\\n\")\n",
    "\n",
    "best_val_psnr = 0\n",
    "train_losses = []\n",
    "val_psnrs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lr = adjust_learning_rate(optimizer, epoch, base_lr=5e-4, max_epoch=num_epochs)\n",
    "    \n",
    "    train_loss, train_psnr = train_epoch(\n",
    "        model, train_loader, optimizer, device,\n",
    "        epoch+1, resolution=resolution\n",
    "    )\n",
    "    \n",
    "    val_loss, val_psnr = validate(\n",
    "        model, test_loader, device, resolution=resolution\n",
    "    )\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_psnrs.append(val_psnr)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} | LR: {lr:.6f}\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, PSNR: {train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, PSNR: {val_psnr:.2f} dB\")\n",
    "    \n",
    "    if val_psnr > best_val_psnr:\n",
    "        best_val_psnr = val_psnr\n",
    "        torch.save(model.state_dict(), 'mamba_ginr_scent_best.pth')\n",
    "        print(f\"  → Best model saved! (PSNR: {best_val_psnr:.2f} dB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation PSNR: {best_val_psnr:.2f} dB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Super-Resolution Test\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('mamba_ginr_scent_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get test images\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images[:8].to(device)\n",
    "\n",
    "print(\"Testing super-resolution with SCENT-style decoder...\\n\")\n",
    "\n",
    "# Super-resolve at multiple resolutions\n",
    "sr_64 = super_resolve(model, test_images, target_resolution=64, device=device)\n",
    "sr_128 = super_resolve(model, test_images, target_resolution=128, device=device)\n",
    "sr_256 = super_resolve(model, test_images, target_resolution=256, device=device)\n",
    "\n",
    "print(f\"Original: {test_images.shape}\")\n",
    "print(f\"SR 64×64: {sr_64.shape}\")\n",
    "print(f\"SR 128×128: {sr_128.shape}\")\n",
    "print(f\"SR 256×256: {sr_256.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original 32×32\n",
    "    axes[0, i].imshow(test_images[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('32×32', fontsize=12, rotation=0, labelpad=30)\n",
    "    \n",
    "    # SR 64×64\n",
    "    axes[1, i].imshow(sr_64[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('64×64 SR', fontsize=12, rotation=0, labelpad=30)\n",
    "    \n",
    "    # SR 128×128\n",
    "    axes[2, i].imshow(sr_128[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('128×128 SR', fontsize=12, rotation=0, labelpad=30)\n",
    "    \n",
    "    # SR 256×256\n",
    "    axes[3, i].imshow(sr_256[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel('256×256 SR', fontsize=12, rotation=0, labelpad=30)\n",
    "\n",
    "plt.suptitle('SCENT-Style Decoder: Super-Resolution Results', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('scent_decoder_superresolution_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Super-resolution visualization saved as 'scent_decoder_superresolution_results.png'\")\n",
    "print(\"\\nExpected improvements over ResidualBlock version:\")\n",
    "print(\"  ✓ Sharp high-frequency details (not smooth blur)\")\n",
    "print(\"  ✓ Better texture synthesis\")\n",
    "print(\"  ✓ More realistic edges and patterns\")\n",
    "print(\"  ✓ +5-10 dB PSNR improvement at 128×128\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}