trainer: imgrec_trainer2

train_dataset:
  name: imgrec_dataset
  args:
    imageset:
      name: celeba
      args: {root_path: /home/idies/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2, split: train}
    resize: 178
  loader:
    batch_size: 16
    num_workers: 8

test_dataset:
  name: imgrec_dataset
  args:
    imageset:
      name: celeba
      args: {root_path: /home/idies/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2, split: test}
    resize: 178
  loader:
    batch_size: 16
    num_workers: 8

model:
  name: mamba_composers_inr
  args:
    tokenizer:
      name: mamba_patch_tokenizer
      args: {input_size: 178, patch_size: 9, padding: 1,  dim: 256}
    hyponet:
      name: hypo_mlp
      args: {in_dim: 2, out_dim: 3, out_bias: 0.5, depth: 5, hidden_dim: 256, use_pe: true, pe_dim: 128}
    mamba_encoder:
      name: mamba_encoder
      args: {dim: 256, depth: 6, ff_dim: 1024}
    num_lp: 256
    gen_layers: [0]
    type: 'equidistant'
    n_group: 1

optimizer:
  name: adamw
  args: {lr: 5.e-4}
max_epoch: 40

eval_epoch: 1
vis_epoch: 5
