trainer: fmri_trainer_lainr_ddp_2d

data_module:
  name: fmri_datamodule
  args:
    config_path: /pscratch/sd/t/tbalasoo/MAMBAINR/trans-inr-master/cfgs/hcp_data_all_2d_config.yaml

model:
  name: lainr_inr_ddp
  args:
    tokenizer:
      name: mamba_patch_tokenizer_fmri
      args: {input_size: 96, patch_size: 6, dim: 256, pos_emb: 'learned'}
    hyponet:
      name: lainr_mlp_bias_fmri_2d
      args: {feature_dim: 256, input_dim: 2, output_dim: 1, sigma_q: 128, sigma_ls: [128, 32], n_patches: 256}
    transformer_encoder:
      name: transformer_encoder
      args: {dim: 256, depth: 6, ff_dim: 1024, n_head: 12, head_dim: 64}
    num_lp: 256
    
optimizer:
  name: adam
  args: {lr: 1.e-4}
max_epoch: 200

eval_epoch: 1
vis_epoch: 1

use_torch_compile: False
use_amp: True
use_augmentation: False