{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa46c84-2203-42ca-9033-b1a02d904db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import einops\n",
    "import torchvision\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch.nn.functional as F\n",
    "import umap.umap_ as umap\n",
    "from utils import make_coord_grid\n",
    "import yaml\n",
    "from datasets import make as make_dataset\n",
    "from models import make as make_model\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from datasets.fmri_dataloader import DataModule\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.modules.block import Block\n",
    "from mamba_ssm.ops.triton.layer_norm import RMSNorm\n",
    "import math\n",
    "from patchembed import PatchEmbed\n",
    "from utils import make_coord_grid\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78687f8d-c230-4a2e-b7c1-d3d6525e0b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f2c16e-9476-448a-8905-4117bba901cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule('./cfgs/hcp_data_all_config_explore.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88442e6f-780c-45d8-8e62-c2a90a9bda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached subject dictionary from data/subj_dict/S1200_age_HCP_filtered_run1_MNI_to_TRs.pickle\n",
      "Randomly determining splits and saving to ./data/splits/S1200_split100.pkl\n",
      "Randomly sampling 400 of 863 subjects.\n",
      "Generating dataset list from scratch...\n",
      "Number of subjects for S1200 'train': 400\n",
      "Randomly sampling 50 of 107 subjects.\n",
      "Loading cached dataset list from data/data_tuple/age_HCP_filtered_run1_MNI_to_TRs_val_seqlen2_within1_between1.0.csv\n",
      "Cache is stale (subject mismatch). Regenerating...\n",
      "Generating dataset list from scratch...\n",
      "Number of subjects for S1200 'val': 50\n",
      "Randomly sampling 50 of 109 subjects.\n",
      "Loading cached dataset list from data/data_tuple/age_HCP_filtered_run1_MNI_to_TRs_test_seqlen2_within1_between1.0.csv\n",
      "Cache is stale (subject mismatch). Regenerating...\n",
      "Generating dataset list from scratch...\n",
      "Number of subjects for S1200 'test': 50\n",
      "\n",
      "Total training segments: 4800\n",
      "Total validation segments: 600\n",
      "Total test segments: 600\n"
     ]
    }
   ],
   "source": [
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "test_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2caa72d-5c81-4b29-b6b9-0ceef133a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    fmri, subj, target_value, tr, sex = batch.values()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea0034-d8e3-49ab-89ee-28b847d1ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0923a4e7-83b2-4263-9445-ad2e1d0e6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAINRDecoder(nn.Module):\n",
    "    def __init__(self, feature_dim, input_dim, output_dim, sigma_q, sigma_ls, n_patches, hidden_dim = 256, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.layer_num = len(sigma_ls)\n",
    "        self.n = feature_dim//(2*input_dim)\n",
    "        self.omegas = torch.logspace(1, math.log10(sigma_q), self.n)\n",
    "        self.patch_num = int(math.sqrt(n_patches))\n",
    "        \n",
    "        '''self.omegas_l = {str(sig_l):torch.logspace(1, math.log10(sig_l), self.n) for sig_l in sigma_ls}\n",
    "        self.bandwidth_lins_lins = nn.ModuleDict({\n",
    "            str(sig_l): nn.Linear(feature_dim, hidden_dim) for sig_l in sigma_ls\n",
    "        })\n",
    "        self.modulation_lins = nn.ModuleDict({\n",
    "            str(sig_l): nn.Linear(hidden_dim, hidden_dim) for sig_l in sigma_ls\n",
    "        })'''\n",
    "\n",
    "        self.alpha = 10.0\n",
    "\n",
    "        self.omegas_l = [torch.logspace(1, math.log10(sigma_ls[i]), self.n) for i in range(self.layer_num)]\n",
    "\n",
    "        self.query_lin = nn.Linear(feature_dim, hidden_dim)\n",
    "\n",
    "        #self.modulation_ca = PerPixelCrossAttention(query_dim = hidden_dim, heads=2)\n",
    "        self.modulation_ca = SharedTokenCrossAttention(query_dim = hidden_dim, heads=2)\n",
    "        \n",
    "        self.bandwidth_lins = nn.ModuleList([\n",
    "            nn.Linear(feature_dim, hidden_dim) for i in range(self.layer_num)\n",
    "                                       ])\n",
    "        \n",
    "        self.modulation_lins = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for i in range(self.layer_num)\n",
    "        ])\n",
    "\n",
    "        self.hv_lins = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(len(sigma_ls) - 1)\n",
    "        ])\n",
    "\n",
    "        self.out_lins = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, output_dim) for _ in range(len(sigma_ls))\n",
    "        ])\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def calc_gamma(self, x, omegas):\n",
    "        #x is passed as H*W, D\n",
    "        L = x.shape[0]\n",
    "        coords = x.unsqueeze(-1)  # (H*W, 2, 1)\n",
    "        omegas = omegas.view(1, 1, -1).to(x.device)  # (1, 1, F)\n",
    "        \n",
    "        \n",
    "        arg = torch.pi * coords * omegas  # shape: (B, 2, F)\n",
    "        sin_part = torch.sin(arg)\n",
    "        cos_part = torch.cos(arg)\n",
    "        \n",
    "        gamma = torch.cat([sin_part, cos_part], dim=-1).view(L, -1)  \n",
    "        \n",
    "        return gamma\n",
    "\n",
    "    def get_patch_index(self, grid, H, W):\n",
    "        y = grid[:, 0]\n",
    "        x = grid[:, 1]\n",
    "        row = (y * H).to(torch.int32)\n",
    "        col = (x * W).to(torch.int32)\n",
    "        return row * W + col  # index in [0, N-1]\n",
    "\n",
    "    def approximate_relative_distances(self, target_index, H, W, m):\n",
    "        alpha = self.alpha\n",
    "        N = H * W  \n",
    "        t = target_index / N\n",
    "        token_positions = torch.tensor([(i + 0.5) / m for i in range(m)])\n",
    "    \n",
    "        rel_distances = -1*alpha*torch.stack([torch.abs((t - s)**2) for s in token_positions], dim = 0)\n",
    "        return rel_distances\n",
    "\n",
    "        \n",
    "    def forward(self, x, tokens):\n",
    "        B, query_shape = x.shape[0], x.shape[1: -1]\n",
    "        x = x.view(B, -1, x.shape[-1]) #B, HW, 2\n",
    "        grid = x[0]\n",
    "        indexes = self.get_patch_index(grid, self.patch_num, self.patch_num)\n",
    "        rel_distances = self.approximate_relative_distances(indexes, self.patch_num, self.patch_num, len(tokens[0]))\n",
    "        bias = rel_distances.transpose(1, 0)\n",
    "        bias = einops.repeat(bias, 'l n -> b l n', b=B) #B, L, HW\n",
    "        x_q = einops.repeat(self.calc_gamma(x[0], self.omegas), 'l d -> b l d', b=B) #B, HW, input_dim\n",
    "        x_q = self.act(self.query_lin(x_q))\n",
    "\n",
    "\n",
    "        #(B, HW, 1, D)\n",
    "        #(B, HW, L, D)\n",
    "\n",
    "        \n",
    "        '''tokens = einops.repeat(tokens, 'b l d -> b p l d', p=query_shape[0]*query_shape[1])\n",
    "        modulation_vector = self.modulation_ca(x_q.unsqueeze(2), context = tokens).squeeze(2)'''\n",
    "        modulation_vector = self.modulation_ca(x_q, context = tokens, bias = bias)\n",
    "\n",
    "        modulations_l = []\n",
    "        h_f = []\n",
    "\n",
    "\n",
    "        for k in range(self.layer_num):\n",
    "            x_l = einops.repeat(self.calc_gamma(x[0], self.omegas_l[k]), 'l d -> b l d', b=B)\n",
    "            h_l = self.act(self.bandwidth_lins[k](x_l))\n",
    "            h_f.append(h_l)\n",
    "            m_l = self.act(h_l + self.modulation_lins[k](modulation_vector))\n",
    "            modulations_l.append(m_l)\n",
    "\n",
    "        h_v = [modulations_l[0]]\n",
    "\n",
    "        for i in range(self.layer_num - 1):\n",
    "            h_vl = self.act(self.hv_lins[i](modulations_l[i+1] + h_v[i]))\n",
    "            h_v.append(h_vl)\n",
    "\n",
    "        outs = [self.out_lins[i](h_v[i]) for i in range(self.layer_num)]\n",
    "\n",
    "        out = sum(outs)\n",
    "        out = out.view(B, *query_shape, -1)  # (B, H, W, output_dim)\n",
    "\n",
    "        return out\n",
    "\n",
    "# helpers\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache = True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "# helper classes\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SharedTokenCrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=2, dim_head=64):\n",
    "        super().__init__()\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context, bias=None):\n",
    "        # x: (B, HW, D)         ← 1 query per pixel (you can squeeze that 1)\n",
    "        # context: (B, L, D)       ← shared tokens\n",
    "\n",
    "        B, HW, D = x.shape\n",
    "\n",
    "        H = self.heads\n",
    "        Dh = self.dim_head\n",
    "        D_inner = H * Dh\n",
    "\n",
    "        q = self.to_q(x)              # (B, HW, H*Dh)\n",
    "        kv = self.to_kv(context)      # (B, L, 2*H*Dh)\n",
    "        k, v = kv.chunk(2, dim=-1)    # (B, L, H*Dh)\n",
    "\n",
    "        # Reshape\n",
    "        q = q.view(B, HW, H, Dh).transpose(1, 2)   # (B, H, HW, Dh)\n",
    "        k = k.view(B, -1, H, Dh).transpose(1, 2)   # (B, H, L, Dh)\n",
    "        v = v.view(B, -1, H, Dh).transpose(1, 2)   # (B, H, L, Dh)\n",
    "\n",
    "        # Attention\n",
    "        sim = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # (B, H, HW, L)\n",
    "        if bias != None:\n",
    "            bias = einops.repeat(bias, 'b l n -> b h l n', h=H) #B, L, HW\n",
    "            sim += bias\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)                              # (B, H, HW, Dh)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, HW, D_inner)  # (B, HW, H*Dh)\n",
    "        out = self.to_out(out)                                        # (B, HW, D)\n",
    "        return out                                   # (B, HW, 1, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d049da-5f59-468a-b646-57be679b6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiMamba(torch.nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super(BiMamba, self).__init__()\n",
    "        \n",
    "        self.f_mamba = Mamba(d_model = dim)\n",
    "        self.r_mamba = Mamba(d_model = dim)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        x_f = self.f_mamba(x, **kwargs)\n",
    "        x_r = torch.flip(self.r_mamba(torch.flip(x, dims=[1]), **kwargs), dims=[1])\n",
    "        out = (x_f + x_r)/2\n",
    "        \n",
    "        return out\n",
    "   \n",
    "class MambaEncoder(torch.nn.Module):\n",
    "    def __init__(self, depth = 6, dim = 768, ff_dim = None, dropout=0.):\n",
    "        super(MambaEncoder, self).__init__()\n",
    "        if not ff_dim:\n",
    "            self.ff_dim = 4*dim\n",
    "        else: \n",
    "            self.ff_dim = ff_dim\n",
    "        token_dim = dim\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=token_dim,\n",
    "                mixer_cls= lambda dim: BiMamba(dim),\n",
    "                mlp_cls= lambda dim: torch.nn.Sequential(\n",
    "                    nn.Linear(dim, self.ff_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(self.ff_dim, dim),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                norm_cls= nn.LayerNorm,  # or RMSNorm, \n",
    "                fused_add_norm=False\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = None\n",
    "        for block in self.blocks:\n",
    "            x, residual = block(x, residual=residual)\n",
    "        return x\n",
    "        \n",
    "def init_wb(shape):\n",
    "    weight = torch.empty(shape[1], shape[0] - 1)\n",
    "    nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n",
    "\n",
    "    bias = torch.empty(shape[1], 1)\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weight)\n",
    "    bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "    nn.init.uniform_(bias, -bound, bound)\n",
    "\n",
    "    return torch.cat([weight, bias], dim=1).t().detach()\n",
    "\n",
    "class MambaInr(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hyponet, mamba_encoder, tokenizer = None, num_lp = 128, type = 'equidistant', n_group = 1, latent_token_len = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.dim = mamba_encoder['args']['dim'] #replace with mamba_encoder\n",
    "        self.dim = 64\n",
    "        self.latent_token_len = latent_token_len\n",
    "        #self.tokenizer = models.make(tokenizer, args={'dim': self.dim}) #replace w correct tokenizer\n",
    "        #self.hyponet = models.make(hyponet, args={'hidden_dim': self.dim})\n",
    "        self.hyponet = hyponet\n",
    "        self.mamba_encoder = mamba_encoder\n",
    "\n",
    "        #self.mamba_encoder = models.make(mamba_encoder) #replace w mamba\n",
    "        #self.input_len = self.tokenizer.n_patches\n",
    "        self.input_len = 432\n",
    "        #self.gen_layers = gen_layers\n",
    "        self.type = type\n",
    "        self.num_lp = num_lp\n",
    "        self.patchifier = PatchEmbed(img_size=(96, 96, 96, 2),\n",
    "        patch_size=(16, 16, 16, 1), embed_dim = 64)\n",
    "        self.n_group = n_group\n",
    "\n",
    "                \n",
    "            \n",
    "        self.lps = nn.Parameter(torch.randn(self.num_lp, self.dim))\n",
    "        self.lp_idxs = None\n",
    "        self.set_lp_idxs(self.input_len, type = self.type, n = self.n_group)\n",
    "        self.perm = self.compute_interleave_permutation(self.input_len, self.num_lp)\n",
    "\n",
    "    def set_lp_idxs(self, seq_len, type = 'equidistant', n = 1):\n",
    "        total_len = seq_len + self.num_lp\n",
    "        if type == 'equidistant':\n",
    "            insert_idxs = torch.linspace(0, total_len - 1, steps=self.num_lp).long()\n",
    "            self.lp_idxs = insert_idxs\n",
    "        elif type == 'middle':\n",
    "            insert_idxs = (np.array(range(self.num_lp))+(seq_len//2)).tolist()\n",
    "            self.lp_idxs = insert_idxs\n",
    "        elif type == 'n_group':\n",
    "            if self.num_lp%n != 0:\n",
    "                raise Exception(\"n must divide number of lps evenly\")\n",
    "            insert_idxs = []\n",
    "            pre_idxs = torch.linspace(0, total_len - n, steps=self.num_lp//n).long()\n",
    "            for idx in pre_idxs:\n",
    "                insert_idxs.extend([idx+i for i in range(n)])\n",
    "            self.lp_idxs = insert_idxs\n",
    "            \n",
    "\n",
    "    def add_lp(self, x):\n",
    "        B, L, D = x.shape\n",
    "        w = einops.repeat(self.lps, 'n d -> b n d', b=B)  # (B, N, D)\n",
    "        x_full = torch.cat([x, w], dim=1)  # (B, L + N, D)\n",
    "        x_perm = x_full[:, self.perm]  # (B, L + N, D) — interleaved\n",
    "\n",
    "        return x_perm\n",
    "    \n",
    "    def extract_lp_tokens(self, x):\n",
    "        return x[:, self.lp_idxs]\n",
    "\n",
    "    def compute_interleave_permutation(self, seq_len, n_insert):\n",
    "        total_len = seq_len + n_insert\n",
    "        insert_idxs = torch.linspace(0, total_len - 1, steps=n_insert).long()  \n",
    "        token_ids = torch.arange(seq_len + n_insert)\n",
    "        perm = torch.full((total_len,), -1, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        perm[insert_idxs] = torch.arange(seq_len, seq_len + n_insert)\n",
    "        input_token_ids = torch.arange(seq_len)\n",
    "        perm[perm == -1] = input_token_ids\n",
    "        \n",
    "        return perm\n",
    "\n",
    "    def scan(self, x):\n",
    "        \n",
    "        B, pD, pH, pW, pT, D = x.shape\n",
    "        x = x.permute(0, 4, 1, 2, 3, 5)  # (B, pT, pD, pH, pW, D)\n",
    "        #x = x.permute(0, 2, 3, 4, 1, 5)\n",
    "    \n",
    "        # Flatten spatial+time dims into one dimension\n",
    "        x = x.reshape(B, pD*pH*pW*pT, D)\n",
    "        return x\n",
    "\n",
    "    def forward(self, data, coord):\n",
    "        all_data = data.float() # (B, C, Z, H, W, T)\n",
    "        B = all_data.shape[0]\n",
    "        dtokens = self.patchifier(all_data) #(B, pD, pH, pW, pT, D)\n",
    "        dtokens = self.scan(dtokens) # (B, pD*pH*pW*pT, D)\n",
    "        #dtokens = self.tokenizer(data)\n",
    "        #Here, delete patches and keep track of which ones are deleted. Can create the pixel mask here too and return it\n",
    "        #lps = einops.repeat(self.lps, 'n d -> b n d', b=B)\n",
    "        all_tokens = self.add_lp(dtokens)\n",
    "        mamba_out = self.mamba_encoder(all_tokens)\n",
    "        mamba_out = self.extract_lp_tokens(mamba_out)\n",
    "        pred = self.hyponet(coord, mamba_out)\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd09b45-d155-4914-8ea1-f592eec42e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6, 6, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "hyponet = LAINRDecoder(feature_dim = 64, input_dim = 4, output_dim = 1, sigma_q = 16, sigma_ls = [128, 32], hidden_dim = 64, n_patches = 432).to(device) #6x6x6x2\n",
    "mamba_encoder = MambaEncoder(dim = 64).to(device)\n",
    "MambaINR = MambaInr(hyponet, mamba_encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb1dddbe-bbde-491f-bd5d-45742fe24d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 96, 96, 2, 4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "gt = fmri[:2].to(device)\n",
    "B = gt.shape[0]\n",
    "coord = make_coord_grid(gt.shape[-4:], (0, 1), device=gt.device).to(device)\n",
    "#coord = self.add_gaussian_noise_to_grid(coord)\n",
    "coord = einops.repeat(coord, 'z h w t d -> b z h w t d', b=B)\n",
    "print(coord.shape)\n",
    "\n",
    "pred = MambaINR(gt, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a5f2976-6378-449d-b311-85c2ac463e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 96, 96, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-env",
   "language": "python",
   "name": "mamba-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
