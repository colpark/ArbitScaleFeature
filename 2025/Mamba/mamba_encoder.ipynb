{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7994684e-06b5-48e8-886e-b3b3a91e3b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from einops import rearrange, repeat\n",
    "import einops\n",
    "from glob import glob\n",
    "from math import log\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm.modules.block import Block\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer import TransformerEncoderINR\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a095377-b677-4d5c-ba10-d609768ffd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae2fc8f-9496-4681-9e58-30591974398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_encode(xy: torch.Tensor, freq: torch.Tensor) -> torch.Tensor:\n",
    "    xy = xy.unsqueeze(1)\n",
    "    freq1 = freq[0:len(freq)//2 ]\n",
    "    freq2 = freq[len(freq)//2:]\n",
    "    \n",
    "    \n",
    "    freq1 = torch.tensor(freq1, dtype = torch.float32).view(1, -1, 1)\n",
    "    freq2 = torch.tensor(freq2, dtype = torch.float32).view(1, -1, 1)\n",
    "\n",
    "    scaled1 = 2 * torch.pi * (1/freq1) * xy  \n",
    "    scaled2 = 2 * torch.pi * (1/freq2) * xy  \n",
    "\n",
    "    sin_feat = torch.sin(scaled1)  \n",
    "    cos_feat = torch.cos(scaled2)  \n",
    "\n",
    "\n",
    "    features = torch.cat([sin_feat, cos_feat], dim=-1)  \n",
    "    return features.view(xy.shape[0], -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b7d09e-8bc1-4f64-9ec5-203379535011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset to customize type of positional encoding of input\n",
    "class ImageINRDatasetFourier(Dataset):\n",
    "    def __init__(self, dataframe, fourier = False, gaussian = False, num_freq = 1, freq_param = None, freq_file = 'freq.pkl', gauss_scale = 1):\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        if (fourier or gaussian) == False:\n",
    "            #No positional encoding, just x and y \n",
    "            self.x = torch.tensor(dataframe[['x_pos', 'y_pos']].values, dtype=torch.float32)\n",
    "        else:\n",
    "            if fourier == True:\n",
    "                #fourier encoding with just cos/sin 2x and 2y\n",
    "                freq = [1, 1]\n",
    "            elif gaussian == True:\n",
    "                #normal random sample num_freq frequencies for cos and sin to include in positional encoding\n",
    "                freq = np.random.normal(0, 1, size = num_freq*2)*gauss_scale\n",
    "\n",
    "                #Need to save these frequencies because they must also be used for pos encoding inputs at inference time\n",
    "                pickle.dump(freq, open(freq_file, \"wb\"))\n",
    "            else:\n",
    "                #if you want to pass in custom frequencies\n",
    "                freq = freq_param\n",
    "\n",
    "            x = dataframe['x_pos']\n",
    "            y = dataframe['y_pos'] \n",
    "            xy = torch.tensor(np.stack([x, y], axis=-1), dtype = torch.float32)\n",
    "            self.x = fourier_encode(xy, freq)\n",
    "                         \n",
    "        self.y = torch.tensor(dataframe[['R', 'G', 'B']].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92d080a-80a8-497b-9753-16135ca7d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(x):\n",
    "        return rearrange(x.reshape(x.shape[0], x.shape[1], -1), 'b c l -> b l c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95a3d4e-38ce-48a8-9034-b30ab8d64c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 400, 3])\n"
     ]
    }
   ],
   "source": [
    "B = 2\n",
    "C = 3\n",
    "H = 20\n",
    "W = 20\n",
    "shape = (B, C, H, W)\n",
    "input = torch.rand(shape)\n",
    "input = scan(input)\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c90230-c129-414e-a1ee-db092eb09724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([2, 400, 32])\n",
      "torch.Size([2, 400, 35])\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 1, W)\n",
    "y = np.linspace(0, 1, H)\n",
    "xx, yy = np.meshgrid(x, y)  # shape: (H, W)\n",
    "\n",
    "X = torch.tensor(np.stack([xx, yy], axis=-1).reshape(-1, 2), dtype = torch.float32)  # shape: (H*W, 2)\n",
    "\n",
    "num_freq = 10\n",
    "gauss_scale = 10\n",
    "#freq = np.random.normal(0, 1, size = num_freq*2)*gauss_scale\n",
    "freq = [1/num for num in range(1, 17)]\n",
    "X = fourier_encode(X, freq)\n",
    "#X = X.reshape(H, W, -1)\n",
    "\n",
    "print(X[0].shape)\n",
    "X = X.repeat(B, 1, 1)\n",
    "print(X.shape)\n",
    "\n",
    "output = torch.cat((X, input), dim = 2)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee02d52b-5461-47fd-95a5-8b0df638efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from models import register\n",
    "\n",
    "#@register('mamba_tokenizer')\n",
    "class MambaTokenizer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, dim, pos_emb = 'learned', padding=0, img_channels=3):\n",
    "        super().__init__()\n",
    "        #Take input of size MxM, flatten to M^2, create gauss pos emb and map to higher dim, add N LPs \n",
    "        \n",
    "        if isinstance(input_size, int):\n",
    "            input_size = (input_size, input_size)\n",
    "\n",
    "        self.learned_posemb = (pos_emb == 'learned')\n",
    "\n",
    "        if (self.learned_posemb):\n",
    "            self.posemb = nn.Parameter(torch.randn(input_size[0]*input_size[1], dim))\n",
    "        else:\n",
    "            self.posemb = self.generate_posemb(input_size[0], input_size[1])\n",
    "            dim -= 32\n",
    "\n",
    "        self.prefc = nn.Linear(img_channels, dim)\n",
    "            \n",
    "\n",
    "    def scan(x):\n",
    "        return rearrange(x.reshape(x.shape[0], x.shape[1], -1), 'b c l -> b l c')\n",
    "\n",
    "    def generate_posemb(self, H, W):\n",
    "        x = np.linspace(0, 1, W)\n",
    "        y = np.linspace(0, 1, H)\n",
    "        xx, yy = np.meshgrid(x, y)  # shape: (H, W)\n",
    "        \n",
    "        X = torch.tensor(np.stack([xx, yy], axis=-1).reshape(-1, 2), dtype = torch.float32)  # shape: (H*W, 2)\n",
    "        \n",
    "        num_freq = 10\n",
    "        gauss_scale = 10\n",
    "        freq = [1/num for num in range(1, 17)]\n",
    "        X = fourier_encode(X, freq)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        #x = data['inp'] #B C H W\n",
    "        x = data #B C H W\n",
    "        B, C, H, W = x.shape\n",
    "        x = scan(x)\n",
    "        x = self.prefc(x)\n",
    "        if (self.learned_posemb):\n",
    "            x = x + self.posemb.unsqueeze(0)\n",
    "        else:\n",
    "            x = torch.cat((self.posemb.repeat(B, 1, 1), x), dim = 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c77f760-beb7-41da-bde6-92887812d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, n_head, head_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        inner_dim = n_head * head_dim\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, fr, to=None):\n",
    "        if to is None:\n",
    "            to = fr\n",
    "        q = self.to_q(fr)\n",
    "        k, v = self.to_kv(to).chunk(2, dim=-1)\n",
    "        q, k, v = map(lambda t: einops.rearrange(t, 'b n (h d) -> b h n d', h=self.n_head), [q, k, v])\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = F.softmax(dots, dim=-1) # b h n n\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = einops.rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, ff_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5603fe5-ef63-401c-b6a0-e1d6aad35d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiMamba(torch.nn.Module):\n",
    "    def __init__(self, dim = 512):\n",
    "        super(BiMamba, self).__init__()\n",
    "        \n",
    "        self.f_mamba = Mamba(d_model = dim)\n",
    "        self.r_mamba = Mamba(d_model = dim)\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        x_f = self.f_mamba(x, **kwargs)\n",
    "        x_r = torch.flip(self.r_mamba(torch.flip(x, dims=[1]), **kwargs), dims=[1])\n",
    "        out = (x_f + x_r)/2\n",
    "        \n",
    "        return out\n",
    "        \n",
    "class MambaStack(torch.nn.Module):\n",
    "    def __init__(self, depth = 3, dim = 512, ff_dim = None, dropout=0.):\n",
    "        super(MambaStack, self).__init__()\n",
    "        if not (ff_dim):\n",
    "            self.ff_dim = 4*dim\n",
    "        else: \n",
    "            self.ff_dim = ff_dim\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                t_dim=token_dim,\n",
    "                mixer_cls= lambda t_dim: BiMamba(dim),\n",
    "                mlp_cls= lambda t_dim: torch.nn.Sequential(\n",
    "                    nn.Linear(t_dim, ff_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(ff_dim, t_dim),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                norm_cls=nn.LayerNorm,  # or RMSNorm\n",
    "                fused_add_norm=False\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = None\n",
    "        for block in self.blocks:\n",
    "            x, residual = block(x, residual=residual)\n",
    "        return x\n",
    "\n",
    "#@register('transformer_encoder')\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, depth, n_head, head_dim, ff_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, n_head, head_dim, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, ff_dim, dropout=dropout)),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for norm_attn, norm_ff in self.layers:\n",
    "            x = x + norm_attn(x)\n",
    "            x = x + norm_ff(x)\n",
    "        return x\n",
    "    \n",
    "class mamba_inr(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, token_dim = 512, output_size = 3, model_type = 'stacked', num_lp = 1):\n",
    "        super(MambaCLS, self).__init__()\n",
    "        self.token_dim = token_dim\n",
    "        if model_type == 'stacked':\n",
    "            self.mamba = MambaStack(num = 6, token_dim = self.token_dim)\n",
    "        else:\n",
    "            self.mamba = BiMamba(token_dim = self.token_dim)\n",
    "            \n",
    "        self.input = torch.nn.Linear(input_size, self.token_dim)\n",
    "        self.output = torch.nn.Linear(self.token_dim, output_size)\n",
    "        self.pred_out= torch.nn.Sequential(torch.nn.Linear(self.token_dim, self.token_dim), torch.nn.Linear(self.token_dim, self.token_dim),\n",
    "                                               torch.nn.Linear(self.token_dim, output_size))\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "        self.num_lp = num_lp\n",
    "        self.lp = torch.nn.Parameter(torch.empty((self.num_lp, self.token_dim), dtype = torch.float32))\n",
    "        self.lp_idxs = None\n",
    "    \n",
    "    def set_lp_idxs(self, lp_idxs):\n",
    "        self.lp_idxs = lp_idxs\n",
    "        \n",
    "    def add_lp(self, x):\n",
    "        \n",
    "        if x.ndim == 2:\n",
    "            seq_len = x.shape[0]\n",
    "        elif x.ndim == 3:\n",
    "            seq_len = x.shape[1]\n",
    "        total_len = seq_len + self.num_lp\n",
    "        \n",
    "        chunk_size = round(seq_len/(self.num_lp+1))\n",
    "        insert_idxs = torch.clamp(torch.tensor([(chunk_size+1)*(x+1)-1 for x in range (self.num_lp)]), min = 0, max = total_len-1)\n",
    "        self.set_lp_idxs(insert_idxs)\n",
    "        \n",
    "\n",
    "        mask = torch.zeros(total_len, dtype=torch.bool)\n",
    "        mask[insert_idxs] = True\n",
    "\n",
    "        out = torch.empty((x.shape[0], total_len, self.token_dim), dtype=torch.float32).to(x.device)\n",
    "        out[:, mask] = self.lp\n",
    "        out[:, ~mask] = x\n",
    "        return out\n",
    "    \n",
    "    def extract_lp_tokens(self, x):\n",
    "        return x[:, self.lp_idxs]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input(x)\n",
    "        x = self.add_lp(x)\n",
    "        x = self.mamba(x)\n",
    "        x = self.extract_lp_tokens(x)\n",
    "        x = self.pred_out(x.squeeze(1))\n",
    "\n",
    "        return x\n",
    "        \n",
    "class MambaEncoder(torch.nn.Module):\n",
    "    def __init__(self, depth = 6, dim = 768, ff_dim = None, dropout=0.):\n",
    "        super(MambaEncoder, self).__init__()\n",
    "        if not ff_dim:\n",
    "            self.ff_dim = 4*dim\n",
    "        else: \n",
    "            self.ff_dim = ff_dim\n",
    "        token_dim = dim\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=token_dim,\n",
    "                mixer_cls= lambda dim: BiMamba(dim),\n",
    "                mlp_cls= lambda dim: torch.nn.Sequential(\n",
    "                    nn.Linear(dim, self.ff_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(self.ff_dim, dim),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                norm_cls=nn.LayerNorm,  # or RMSNorm\n",
    "                fused_add_norm=False\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = None\n",
    "        for block in self.blocks:\n",
    "            x, residual = block(x, residual=residual)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b709161-7ec9-4eca-b760-e148ffba0e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100, 768])\n",
      "torch.Size([3, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "T = TransformerEncoder(dim = 768, depth = 6, n_head = 4, head_dim = 64, ff_dim = 3072).to(device)\n",
    "M = MambaEncoder(dim = 768).to(device)\n",
    "B = 3\n",
    "L = 100\n",
    "D = 768\n",
    "\n",
    "test_input = torch.randn((B, L, D)).to(device)\n",
    "\n",
    "out1 = T(test_input)\n",
    "out2 = M(test_input)\n",
    "\n",
    "print(out1.shape)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb49cd6-867e-4eec-950e-049439a211b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:38:10.029450Z",
     "iopub.status.busy": "2025-07-03T15:38:10.029112Z",
     "iopub.status.idle": "2025-07-03T15:38:10.035590Z",
     "shell.execute_reply": "2025-07-03T15:38:10.035137Z",
     "shell.execute_reply.started": "2025-07-03T15:38:10.029428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2638, -0.8965,  0.1289,  0.1090, -1.1527]])\n",
      "tensor([[[-0.2638],\n",
      "         [-0.8965],\n",
      "         [ 0.1289],\n",
      "         [ 0.1090],\n",
      "         [-1.1527]]])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.randn((1, 5))\n",
    "print(test_input)\n",
    "test_input = test_input.unsqueeze(2)\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e844f1-bdc6-4e07-8504-16df84f57b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypoMlp(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, depth, in_dim, out_dim, hidden_dim, use_pe, pe_dim, out_bias=0, pe_sigma=1024):\n",
    "        super().__init__()\n",
    "        self.use_pe = use_pe\n",
    "        self.pe_dim = pe_dim\n",
    "        self.pe_sigma = pe_sigma\n",
    "        self.depth = depth\n",
    "        self.param_shapes = dict()\n",
    "        if use_pe:\n",
    "            last_dim = in_dim * pe_dim\n",
    "        else:\n",
    "            last_dim = in_dim\n",
    "        for i in range(depth):\n",
    "            cur_dim = hidden_dim if i < depth - 1 else out_dim\n",
    "            self.param_shapes[f'wb{i}'] = (last_dim + 1, cur_dim)\n",
    "            last_dim = cur_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.params = None\n",
    "        self.out_bias = out_bias\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def convert_posenc(self, x):\n",
    "        w = torch.exp(torch.linspace(0, np.log(self.pe_sigma), self.pe_dim // 2, device=x.device))\n",
    "        x = torch.matmul(x.unsqueeze(-1), w.unsqueeze(0)).view(*x.shape[:-1], -1)\n",
    "        x = torch.cat([torch.cos(np.pi * x), torch.sin(np.pi * x)], dim=-1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, query_shape = x.shape[0], x.shape[1: -1]\n",
    "        x = x.view(B, -1, x.shape[-1])\n",
    "        if self.use_pe:\n",
    "            x = self.convert_posenc(x)\n",
    "        for i in range(self.depth):\n",
    "            x = batched_linear_mm(x, self.params[f'wb{i}'])\n",
    "            if i < self.depth - 1:\n",
    "                x = self.relu(x)\n",
    "            else:\n",
    "                x = x + self.out_bias\n",
    "        x = x.view(B, *query_shape, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b4f7b7b-5135-4418-a2f9-d3158853e5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyponet = HypoMlp(5, 2, 3, 256, True, 128, out_bias=0, pe_sigma=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "744c6c33-14b5-4c06-b407-f23d728d23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wb0': (257, 256), 'wb1': (257, 256), 'wb2': (257, 256), 'wb3': (257, 256), 'wb4': (257, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(hyponet.param_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3159a4f9-021d-429a-afbc-34bbb7175898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c26231b-4e36-49d4-9a15-29622c330775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='.', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='.', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431ce1f8-0ad8-4d34-834b-121e01c262da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "938\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader))\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96adfd3-a66f-4a0b-a611-639c180fa0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_250/4009651110.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  img1 = torch.tensor(np.array(train_data[0][0]))\n",
      "/tmp/ipykernel_250/4009651110.py:2: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  img2 = torch.tensor(np.array(train_data[1][0]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img1 = torch.tensor(np.array(train_data[0][0]))\n",
    "img2 = torch.tensor(np.array(train_data[1][0]))\n",
    "\n",
    "img = torch.cat((img1.unsqueeze(0), img2.unsqueeze(0)), dim = 0)\n",
    "print(img.shape)\n",
    "plt.imshow(img1.squeeze(0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "915bc74b-bdaa-4d4c-ae7d-41553effa028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 256])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MambaTokenizer(28, 256, pos_emb = 'learned', img_channels = 1)\n",
    "out = tokenizer(img)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f5e61f-9616-4b96-9873-e0cba5e54a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:38:15.032194Z",
     "iopub.status.busy": "2025-07-03T15:38:15.031855Z",
     "iopub.status.idle": "2025-07-03T15:38:15.036158Z",
     "shell.execute_reply": "2025-07-03T15:38:15.035683Z",
     "shell.execute_reply.started": "2025-07-03T15:38:15.032171Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  6,  2,  7,  3,  8,  4,  9,  5, 10]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]])\n",
    "x = torch.transpose(x, 1, 2)\n",
    "print(x.reshape(1, 10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b17c4d6-a589-4fec-a015-33ea81337364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scan(x):\n",
    "    return x.reshape(x.shape[0], -1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec64b59-a311-4119-9f98-b47844f98e43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MambaCLS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMambaCLS\u001b[49m(input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, token_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MambaCLS' is not defined"
     ]
    }
   ],
   "source": [
    "model = MambaCLS(input_size = 1, token_dim = 256, output_size = 10).to(device)\n",
    "model.eval()\n",
    "\n",
    "test_input = torch.randn((1, 28, 28)).to(device)\n",
    "test_input = scan(test_input)\n",
    "out = model(test_input)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcec02fe-8e00-4b1d-8047-9513ee8f3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "pickle.dump(losses, open('./cls_losses.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344b1a2d-1a34-4f7e-aed9-384ba435a9da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # ======= Model Setup =======\n",
    "    model_dim = 128\n",
    "    num_classes = 10\n",
    "    ofcount = 0\n",
    "\n",
    "    model = MambaCLS(input_size = 1, token_dim = 256, output_size = num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ======= Training Loop =======\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5              # How many epochs to wait before considering convergence\n",
    "    epochs_no_improve = 0     # Counter for early stopping\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, labels in tqdm(train_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #x = patchify(imgs, patch_size)  # (B, N, D)\n",
    "            x = scan(imgs)\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        losses.append(total_loss / len(train_loader))\n",
    "        pickle.dump(losses, open('./cls_losses.pkl', \"wb\"))\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Eval\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(test_loader):\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                x = scan(imgs)\n",
    "                preds = model(x)\n",
    "                loss = criterion(preds, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                pred_labels = preds.argmax(dim=1)\n",
    "                correct += (pred_labels == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss = total_val_loss/len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        pickle.dump(val_losses, open('./cls_val_losses.pkl', \"wb\"))\n",
    "        acc = correct / total\n",
    "        print(f\"[Epoch {epoch+1}] Test Accuracy: {acc:.2%}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': total_loss,\n",
    "                    # Add other relevant information as needed\n",
    "                }\n",
    "            model_file = f'./mamba_cls_models/mamba_cls_epoch_{epoch}.pth'\n",
    "            torch.save(checkpoint, model_file)\n",
    "            print(\"Validation loss improved. Model saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "    \n",
    "        # Check for convergence\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping: Validation loss has converged.\")\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97254748-ba3b-4496-9dd2-e8f41668d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.8159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Test Accuracy: 80.42%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Test Accuracy: 84.62%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Test Accuracy: 86.12%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.3496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Test Accuracy: 86.29%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:32<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.3215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Test Accuracy: 87.61%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:31<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Test Accuracy: 89.06%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.2811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Test Accuracy: 89.41%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.2895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Test Accuracy: 89.77%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.2523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Test Accuracy: 89.09%\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.2568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Test Accuracy: 90.53%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:31<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.2943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Test Accuracy: 89.72%\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:31<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Test Accuracy: 90.69%\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Test Accuracy: 90.91%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Test Accuracy: 90.16%\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Test Accuracy: 90.70%\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss: 0.2055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Test Accuracy: 91.46%\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss: 0.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Test Accuracy: 91.29%\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss: 0.3655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Test Accuracy: 32.51%\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Loss: 0.7545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Test Accuracy: 80.55%\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [02:30<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Loss: 0.4686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:08<00:00, 18.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Test Accuracy: 83.96%\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambakernel",
   "language": "python",
   "name": "mambakernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
