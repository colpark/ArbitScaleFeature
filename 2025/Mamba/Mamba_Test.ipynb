{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ee2ae5-776b-4ee5-9c26-27f18363bdad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, repeat\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from einops import rearrange, repeat\n",
    "import einops\n",
    "from glob import glob\n",
    "from math import log\n",
    "import math\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01583534-6cf8-4794-968a-c5d7225894ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T01:44:35.728859Z",
     "iopub.status.busy": "2025-06-29T01:44:35.728499Z",
     "iopub.status.idle": "2025-06-29T01:44:35.773168Z",
     "shell.execute_reply": "2025-06-29T01:44:35.772542Z",
     "shell.execute_reply.started": "2025-06-29T01:44:35.728838Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c044ed7a-cda4-4fe8-8690-1b7a9f8b95d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T01:44:40.266986Z",
     "iopub.status.busy": "2025-06-29T01:44:40.266440Z",
     "iopub.status.idle": "2025-06-29T01:44:43.102812Z",
     "shell.execute_reply": "2025-06-29T01:44:43.102215Z",
     "shell.execute_reply.started": "2025-06-29T01:44:40.266951Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 16, 64])\n",
      "Output shape: torch.Size([2, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§© Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm.modules.mamba_simple import Mamba\n",
    "\n",
    "# âœ… Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ðŸ”§ Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "dim = 64  # model dimension\n",
    "\n",
    "# ðŸ“¦ Dummy input\n",
    "x = torch.randn(batch_size, seq_len, dim).to(device)\n",
    "\n",
    "# âš™ï¸ Mamba Block Configuration\n",
    "mamba_block = Mamba(\n",
    "    d_model=dim,\n",
    "    layer_idx=0,\n",
    "    # Optional configs:\n",
    "    # dt_rank=\"auto\",\n",
    "    # d_state=16,\n",
    "    # expand=2,\n",
    ").to(device)\n",
    "\n",
    "# ðŸ§  Forward pass\n",
    "output = mamba_block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2a4ed0-3b72-46f0-a269-771124f9f3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:34:32.296445Z",
     "iopub.status.busy": "2025-06-29T17:34:32.296131Z",
     "iopub.status.idle": "2025-06-29T17:34:32.322892Z",
     "shell.execute_reply": "2025-06-29T17:34:32.322336Z",
     "shell.execute_reply.started": "2025-06-29T17:34:32.296423Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Embedder' from 'mamba_ssm' (/home/idies/workspace/Storage/tbalasoor/persistent/2025/Mamba/mamba/mamba_ssm/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embedder\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RMSNorm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMambaGPT\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Embedder' from 'mamba_ssm' (/home/idies/workspace/Storage/tbalasoor/persistent/2025/Mamba/mamba/mamba_ssm/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm import RMSNorm\n",
    " \n",
    "class MambaGPT(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_layers=12, d_state=64, d_conv=4, expand=2, klen=10, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedder = Embedder(embed_dim = embed_dim, dropout = dropout)\n",
    "        self.mamba_layers = nn.ModuleList(\n",
    "            [nn.Sequential(RMSNorm(embed_dim), \n",
    "                           Mamba(d_model=embed_dim, d_state=d_state, d_conv=d_conv, expand=expand),\n",
    "                           nn.Dropout(dropout)) \n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(embed_dim, klen * 3)\n",
    "        self.norm = RMSNorm(embed_dim)\n",
    " \n",
    "    def change_maskval(self, x, init_val = -100, target_val = 0):\n",
    "        out = x.clone()\n",
    "        out[out == init_val] = target_val\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, return_z = False):\n",
    "        in_scale, out_scale = 1.0, 1.0\n",
    "        x = self.change_maskval(x) # for training stability\n",
    "        x = self.embedder(x)  # Add slight noise\n",
    "        \n",
    "        x = x * in_scale\n",
    "        feature = []\n",
    "        # feature = 0\n",
    "        for layer in self.mamba_layers:\n",
    "            z = layer(x)\n",
    "            feature.append(z)\n",
    "            x = z + x           \n",
    "            \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if return_z:\n",
    "            return self.output_layer(x) * out_scale, feature\n",
    "        else:\n",
    "            return self.output_layer(x) * out_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb61101-859f-4f16-8cd4-14849b944339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:33:34.854242Z",
     "iopub.status.busy": "2025-06-29T17:33:34.853932Z",
     "iopub.status.idle": "2025-06-29T17:33:34.882694Z",
     "shell.execute_reply": "2025-06-29T17:33:34.882143Z",
     "shell.execute_reply.started": "2025-06-29T17:33:34.854223Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMambaGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mMambaGPT.__init__\u001b[0;34m(self, embed_dim, num_layers, d_state, d_conv, expand, klen, dropout)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedder\u001b[49m(embed_dim \u001b[38;5;241m=\u001b[39m embed_dim, dropout \u001b[38;5;241m=\u001b[39m dropout)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     14\u001b[0m     [nn\u001b[38;5;241m.\u001b[39mSequential(RMSNorm(embed_dim), \n\u001b[1;32m     15\u001b[0m                    Mamba(d_model\u001b[38;5;241m=\u001b[39membed_dim, d_state\u001b[38;5;241m=\u001b[39md_state, d_conv\u001b[38;5;241m=\u001b[39md_conv, expand\u001b[38;5;241m=\u001b[39mexpand),\n\u001b[1;32m     16\u001b[0m                    nn\u001b[38;5;241m.\u001b[39mDropout(dropout)) \n\u001b[1;32m     17\u001b[0m      \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, klen \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Embedder' is not defined"
     ]
    }
   ],
   "source": [
    "model = MambaGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9591b49-33ff-4635-84dd-925fae7cb8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
