{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66b03d-c0a5-4583-831f-042df82a11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from einops import rearrange, repeat\n",
    "import ssl\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi, log\n",
    "from functools import wraps\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fix for torchvision dataset download issue\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# ===============================================================\n",
    "# --- 1. The One True Perceiver IO Model Architecture ---\n",
    "# ===============================================================\n",
    "\n",
    "# helpers\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache = True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "# helper classes\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h = self.heads\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "        sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = torch.einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "from math import log\n",
    "\n",
    "# This helper function creates the sinusoidal embeddings\n",
    "def get_sinusoidal_embeddings(n, d):\n",
    "    \"\"\"\n",
    "    Generates sinusoidal positional embeddings.\n",
    "    \n",
    "    Args:\n",
    "        n (int): The number of positions (num_latents).\n",
    "        d (int): The embedding dimension (latent_dim).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (n, d) with sinusoidal embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure latent_dim is even for sin/cos pairs\n",
    "    assert d % 2 == 0, \"latent_dim must be an even number for sinusoidal embeddings\"\n",
    "    \n",
    "    position = torch.arange(n, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d, 2).float() * -(log(10000.0) / d))\n",
    "    \n",
    "    pe = torch.zeros(n, d)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# main class\n",
    "class PerceiverIO(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth,\n",
    "        dim,\n",
    "        queries_dim,\n",
    "        logits_dim = None,\n",
    "        num_latents = 512,\n",
    "        latent_dim = 512,\n",
    "        cross_heads = 1,\n",
    "        latent_heads = 8,\n",
    "        cross_dim_head = 64,\n",
    "        latent_dim_head = 64,\n",
    "        weight_tie_layers = False,\n",
    "        decoder_ff = False,\n",
    "        seq_dropout_prob = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_dropout_prob = seq_dropout_prob\n",
    "\n",
    "        # --- MODIFICATION START ---\n",
    "        # 1. Generate sinusoidal embeddings instead of random noise.\n",
    "        sinu_embeds = get_sinusoidal_embeddings(num_latents, latent_dim)\n",
    "        \n",
    "        # 2. Register 'latents' as a non-trainable buffer instead of a learnable nn.Parameter.\n",
    "        self.register_buffer('latents', sinu_embeds)\n",
    "        # --- MODIFICATION END ---\n",
    "        \n",
    "        self.cross_attend_blocks = nn.ModuleList([\n",
    "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
    "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
    "        ])\n",
    "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
    "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "        self.layers = nn.ModuleList([])\n",
    "        cache_args = {'_cache': weight_tie_layers}\n",
    "        for i in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_latent_attn(**cache_args),\n",
    "                get_latent_ff(**cache_args)\n",
    "            ]))\n",
    "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
    "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
    "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data,\n",
    "        mask = None,\n",
    "        queries = None\n",
    "    ):\n",
    "        b, *_, device = *data.shape, data.device\n",
    "        x = repeat(self.latents, 'n d -> b n d', b = b)\n",
    "        cross_attn, cross_ff = self.cross_attend_blocks\n",
    "        if self.training and self.seq_dropout_prob > 0.:\n",
    "            data, mask = dropout_seq(data, mask, self.seq_dropout_prob)\n",
    "        x = cross_attn(x, context = data, mask = mask) + x\n",
    "        x = cross_ff(x) + x\n",
    "        for self_attn, self_ff in self.layers:\n",
    "            x = self_attn(x) + x\n",
    "            x = self_ff(x) + x\n",
    "        if not exists(queries):\n",
    "            return x\n",
    "        if queries.ndim == 2:\n",
    "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
    "        \n",
    "        latents = self.decoder_cross_attn(queries, context = x)\n",
    "        if exists(self.decoder_ff):\n",
    "            latents = latents + self.decoder_ff(latents)\n",
    "            \n",
    "        return self.to_logits(latents)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# --- Training Script Starts Here ---\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "# Adjusted learning rate to a more standard value for this kind of task\n",
    "LEARNING_RATE = 2e-4\n",
    "# Using an available GPU, change if needed\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMAGE_SIZE_TRAIN = 32\n",
    "IMAGE_SIZE_HI_RES = 128\n",
    "CHANNELS = 3\n",
    "\n",
    "POS_EMBED_DIM = 64\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "LOGITS_DIM = CHANNELS\n",
    "\n",
    "# --- Data Loading ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1]\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "class GaussianFourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, mapping_size, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.mapping_size = mapping_size\n",
    "        self.register_buffer('B', torch.randn((in_features, mapping_size)) * scale)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        projections = coords @ self.B\n",
    "        fourier_feats = torch.cat([torch.sin(projections), torch.cos(projections)], dim=-1)\n",
    "        return fourier_feats\n",
    "\n",
    "# --- In your main script ---\n",
    "FOURIER_MAPPING_SIZE = 96\n",
    "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "\n",
    "fourier_encoder = GaussianFourierFeatures(\n",
    "    in_features=2,\n",
    "    mapping_size=FOURIER_MAPPING_SIZE,\n",
    "    scale=15.0\n",
    ").to(DEVICE)\n",
    "\n",
    "model = PerceiverIO(\n",
    "    depth=6,\n",
    "    dim=INPUT_DIM,\n",
    "    queries_dim=QUERIES_DIM,\n",
    "    logits_dim=LOGITS_DIM,\n",
    "    num_latents=256,\n",
    "    latent_dim=512,\n",
    "    cross_heads=1,\n",
    "    latent_heads=8,\n",
    "    cross_dim_head=64,\n",
    "    latent_dim_head=64,\n",
    "    decoder_ff=True\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(list(model.parameters()) + list(fourier_encoder.parameters()), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS * len(train_loader))\n",
    "\n",
    "print(f\"Training on {DEVICE}\")\n",
    "total_params = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in fourier_encoder.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "# --- Helper Functions and Coordinate Grids ---\n",
    "def create_coordinate_grid(h, w, device):\n",
    "    grid = torch.stack(torch.meshgrid(\n",
    "        torch.linspace(-1.0, 1.0, h, device=device),\n",
    "        torch.linspace(-1.0, 1.0, w, device=device),\n",
    "        indexing='ij'\n",
    "    ), dim=-1)\n",
    "    return rearrange(grid, 'h w c -> (h w) c')\n",
    "\n",
    "coords_32x32 = create_coordinate_grid(IMAGE_SIZE_TRAIN, IMAGE_SIZE_TRAIN, DEVICE)\n",
    "coords_128x128 = create_coordinate_grid(IMAGE_SIZE_HI_RES, IMAGE_SIZE_HI_RES, DEVICE)\n",
    "\n",
    "def prepare_model_input(images, coords, fourier_encoder_fn):\n",
    "    b, c, h, w = images.shape\n",
    "    pixels = rearrange(images, 'b c h w -> b (h w) c')\n",
    "    batch_coords = repeat(coords, 'n d -> b n d', b=b)\n",
    "    pos_embeddings = fourier_encoder_fn(batch_coords)\n",
    "    input_with_pos = torch.cat((pixels, pos_embeddings), dim=-1)\n",
    "    return input_with_pos, pixels, pos_embeddings\n",
    "\n",
    "def imshow(img, title):\n",
    "    img = img.cpu() / 2 + 0.5 # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ===============================================================\n",
    "# --- NEW: FFT Validation Function ---\n",
    "# ===============================================================\n",
    "def calculate_and_visualize_fft_power_delta(original_imgs, recon_imgs, epoch_num):\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the FFT power spectrum difference between\n",
    "    the original and reconstructed images.\n",
    "    \"\"\"\n",
    "    # Use the first image in the batch for visualization\n",
    "    original_img = original_imgs[0]\n",
    "    recon_img = recon_imgs[0]\n",
    "\n",
    "    # Convert to grayscale for 2D FFT\n",
    "    # New versions of torchvision use functional transforms\n",
    "    original_gray = transforms.functional.rgb_to_grayscale(original_img)\n",
    "    recon_gray = transforms.functional.rgb_to_grayscale(recon_img)\n",
    "\n",
    "    # --- FFT Calculation ---\n",
    "    def get_log_power_spectrum(img_tensor):\n",
    "        # Squeeze the channel dimension\n",
    "        img_tensor = img_tensor.squeeze(0)\n",
    "        # Apply 2D FFT\n",
    "        fft = torch.fft.fft2(img_tensor)\n",
    "        # Shift the zero frequency component to the center\n",
    "        fft_shifted = torch.fft.fftshift(fft)\n",
    "        # Calculate the power spectrum (magnitude squared)\n",
    "        power_spectrum = torch.abs(fft_shifted)**2\n",
    "        # Use log scale for better visualization\n",
    "        log_power_spectrum = torch.log1p(power_spectrum)\n",
    "        return log_power_spectrum.cpu().numpy()\n",
    "\n",
    "    original_fft_power = get_log_power_spectrum(original_gray)\n",
    "    recon_fft_power = get_log_power_spectrum(recon_gray)\n",
    "    delta_power = np.abs(original_fft_power - recon_fft_power)\n",
    "\n",
    "    # --- Visualization ---\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(f'Epoch {epoch_num}: FFT Power Spectrum Comparison', fontsize=16)\n",
    "\n",
    "    im1 = axs[0].imshow(original_fft_power, cmap='viridis')\n",
    "    axs[0].set_title('Original Image FFT Power')\n",
    "    axs[0].axis('off')\n",
    "    fig.colorbar(im1, ax=axs[0])\n",
    "\n",
    "    im2 = axs[1].imshow(recon_fft_power, cmap='viridis')\n",
    "    axs[1].set_title('Reconstructed Image FFT Power')\n",
    "    axs[1].axis('off')\n",
    "    fig.colorbar(im2, ax=axs[1])\n",
    "    \n",
    "    im3 = axs[2].imshow(delta_power, cmap='magma')\n",
    "    axs[2].set_title('Power Difference (Delta)')\n",
    "    axs[2].axis('off')\n",
    "    fig.colorbar(im3, ax=axs[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Main Training and Validation Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    fourier_encoder.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        input_data, target_pixels, queries = prepare_model_input(images, coords_32x32, fourier_encoder)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        reconstructed_pixels = model(input_data, queries=queries)\n",
    "        loss = loss_fn(reconstructed_pixels, target_pixels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_train_loss += loss.item()\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], LR: {scheduler.get_last_lr()[0]:.6f}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            input_data, target_pixels, queries = prepare_model_input(images, coords_32x32, fourier_encoder)\n",
    "            reconstructed_pixels = model(input_data, queries=queries)\n",
    "            total_val_loss += loss_fn(reconstructed_pixels, target_pixels).item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f\"--- Epoch [{epoch+1}/{EPOCHS}] Summary ---\")\n",
    "    print(f\"  Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Avg Validation Loss: {avg_val_loss:.4f}\\n\")\n",
    "\n",
    "    # --- Visualization at the end of each epoch ---\n",
    "    with torch.no_grad():\n",
    "        context_images, _ = next(iter(test_loader))\n",
    "        context_images = context_images.to(DEVICE)[:8]\n",
    "        b, c, h, w = context_images.shape\n",
    "\n",
    "        # 1. Low-Resolution Reconstruction Test\n",
    "        input_context, _, queries_context = prepare_model_input(context_images, coords_32x32, fourier_encoder)\n",
    "        reconstructed_pixels = model(input_context, queries=queries_context)\n",
    "        reconstructed_images = rearrange(reconstructed_pixels, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        comparison_grid = torch.cat((context_images, reconstructed_images), dim=0)\n",
    "        final_grid = torchvision.utils.make_grid(comparison_grid, nrow=8, padding=2)\n",
    "        imshow(final_grid, f\"Epoch {epoch+1}: Top: Original | Bottom: Reconstructed (32x32)\")\n",
    "\n",
    "        # ===============================================================\n",
    "        # --- NEW: Call the FFT validation function ---\n",
    "        # ===============================================================\n",
    "        calculate_and_visualize_fft_power_delta(context_images, reconstructed_images, epoch + 1)\n",
    "        # ===============================================================\n",
    "\n",
    "        # 2. High-Resolution Generation Test\n",
    "        high_res_batch_coords = repeat(coords_128x128, 'n d -> b n d', b=b)\n",
    "        high_res_queries = fourier_encoder(high_res_batch_coords)\n",
    "        \n",
    "        generated_pixels = model(input_context, queries=high_res_queries)\n",
    "        generated_images = rearrange(generated_pixels, 'b (h w) c -> b c h w', h=IMAGE_SIZE_HI_RES, w=IMAGE_SIZE_HI_RES)\n",
    "\n",
    "        generated_grid = torchvision.utils.make_grid(generated_images, nrow=4, padding=2)\n",
    "        imshow(generated_grid, f\"Epoch {epoch+1}: Generated High-Resolution Images (128x128)\")\n",
    "\n",
    "print(\"--- Training finished. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambakernel",
   "language": "python",
   "name": "mambakernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
